# NVIDIA GPU优化的Qwen模型配置文件
# Qwen Model Configuration for GPU Fine-tuning (RTX 4060 Optimized)

model:
  name: "Qwen/Qwen2.5-3B-Instruct"  # 使用HuggingFace上的Qwen模型
  model_type: "qwen2"
  torch_dtype: "bfloat16"  # Qwen默认使用bfloat16，GPU数值稳定性更好
  device_map: "auto"
  trust_remote_code: true
  use_cache: false  # 训练时禁用缓存以节省显存
  low_cpu_mem_usage: true  # 降低CPU内存使用

  # GPU特定优化 - RTX 4060兼容
  attn_implementation: "flash_attention_2"  # 使用Flash Attention 2加速
  use_flash_attention_2: true

tokenizer:
  name: "Qwen/Qwen2.5-3B-Instruct"
  padding_side: "left"
  truncation_side: "left"
  add_eos_token: true
  add_bos_token: false  # Qwen通常不需要BOS token

training:
  # 训练超参数 - RTX 4060优化 (8GB显存)
  learning_rate: 1.5e-4  # Qwen对学习率敏感，使用适中值
  num_train_epochs: 3
  per_device_train_batch_size: 2  # RTX 4060 8GB显存，使用较小批次
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 32  # 有效批大小 = 2 * 32 = 64

  # 优化器设置
  optim: "adamw_torch_fused"  # GPU融合优化器，更快
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # 学习率调度
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03

  # GPU内存优化
  gradient_checkpointing: true
  dataloader_pin_memory: true  # GPU环境启用pin_memory
  remove_unused_columns: false
  ddp_find_unused_parameters: false  # DDP优化

  # 混合精度训练
  bf16: true  # 使用bfloat16而不是fp16
  fp16: false
  tf32: true  # 启用TensorFloat-32（A100/H100）

  # 数据加载优化
  dataloader_num_workers: 4  # GPU环境可以使用多线程
  dataloader_prefetch_factor: 2

  # 保存和日志
  save_steps: 500
  eval_steps: 500
  logging_steps: 10
  save_total_limit: 3
  evaluation_strategy: "steps"
  save_strategy: "steps"

  # 其他设置
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  # GPU特定优化
  torch_compile: true  # PyTorch 2.0编译优化
  include_inputs_for_metrics: false

data:
  # 数据集配置
  dataset_name: "tatsu-lab/alpaca"
  max_length: 1024  # RTX 4060适中序列长度，平衡性能和显存
  train_split: "train"
  validation_split: 0.1
  test_split: 0.1

  # Qwen指令模板 - 使用Qwen Chat格式
  instruction_template: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{instruction}\n{input}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
  prompt_template: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{instruction}\n{input}<|im_end|>\n<|im_start|>assistant\n"

paths:
  # 路径配置
  output_dir: "./models/qwen_gpu_checkpoints"
  cache_dir: "./models/cache"
  logging_dir: "./experiments/qwen_gpu_logs"

# RTX 4060特定配置
gpu:
  enabled: true
  use_cuda: true
  cuda_visible_devices: "0"  # 指定使用的GPU
  memory_fraction: 0.90  # RTX 4060使用90%显存，留出余量

  # RTX 4060规格
  gpu_model: "RTX 4060"
  vram_size: "8GB"
  compute_capability: "8.9"  # Ada Lovelace架构

  # 性能优化
  cudnn_benchmark: true  # 启用cuDNN基准测试
  cudnn_deterministic: false  # 为了性能，不强制确定性

  # 显存管理 - RTX 4060优化
  empty_cache_steps: 50   # 更频繁清理显存缓存
  gradient_checkpointing_kwargs:
    use_reentrant: false  # 新版本推荐设置

# RTX 4060配置说明
rtx_4060_config:
  # 硬件规格
  specifications:
    vram: "8GB GDDR6"
    memory_bandwidth: "272 GB/s"
    cuda_cores: 3072
    rt_cores: "24 (3rd gen)"
    tensor_cores: "96 (4th gen)"
    base_clock: "1830 MHz"
    boost_clock: "2460 MHz"

  # 推荐设置
  recommended_settings:
    batch_size: 2
    max_length: 1024
    lora_rank: 16
    gradient_accumulation: 32
    bf16: true
    flash_attention_2: true

  # 性能预期
  performance_expectations:
    training_time: "2-3 hours for 3 epochs"
    memory_usage: "6-7GB VRAM"
    tokens_per_second: "30-50"