# 模型和训练配置文件
# Model Configuration for Llama 3.2-3B Instruction Tuning

model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  model_type: "llama"
  torch_dtype: "float16"  # 使用FP16以节省内存
  device_map: "auto"
  trust_remote_code: true
  use_cache: false  # 训练时禁用缓存以节省内存

tokenizer:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  padding_side: "left"
  truncation_side: "left"
  add_eos_token: true
  add_bos_token: true

training:
  # 训练超参数
  learning_rate: 1.0e-4
  num_train_epochs: 3
  per_device_train_batch_size: 1  # 小批次适配M3 Pro内存
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 64  # 有效批大小 = 1 * 64 = 64

  # 优化器设置
  optim: "adamw_torch"
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # 学习率调度
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03

  # 内存优化
  gradient_checkpointing: true
  dataloader_pin_memory: false
  remove_unused_columns: false

  # 保存和日志
  save_steps: 500
  eval_steps: 500
  logging_steps: 10
  save_total_limit: 3
  evaluation_strategy: "steps"
  save_strategy: "steps"

  # 其他设置
  fp16: true  # 混合精度训练
  dataloader_num_workers: 0  # 单线程数据加载避免内存问题
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

data:
  # 数据集配置
  dataset_name: "tatsu-lab/alpaca"
  max_length: 512  # 最大序列长度
  train_split: "train"
  validation_split: 0.1
  test_split: 0.1

  # 指令模板
  instruction_template: "### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n{output}"
  prompt_template: "### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n"

paths:
  # 路径配置
  output_dir: "./models/checkpoints"
  cache_dir: "./models/cache"
  logging_dir: "./experiments/logs"

mlx:
  # MLX特定配置 (Apple Silicon优化)
  enabled: true
  memory_limit: 14  # GB, 为M3 Pro 18GB内存留出余量