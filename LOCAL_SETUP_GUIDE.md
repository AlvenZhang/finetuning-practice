# æœ¬åœ°æ¨¡å‹å’Œæ•°æ®è®­ç»ƒæŒ‡å—

## ğŸ“‹ ç¯å¢ƒé…ç½®

### âœ… å½“å‰çŠ¶æ€
- **GPU**: NVIDIA RTX 4060 (8GB VRAM)
- **æ¨¡å‹**: Qwen2.5-3B-Instruct (æœ¬åœ°ä¸‹è½½ï¼Œ5.75 GB)
- **æ•°æ®**: Alpacaæ•°æ®é›† (51,760æ ·æœ¬ï¼Œ42.3 MB)
- **é…ç½®**: RTX 4060ä¼˜åŒ–çš„GPUé…ç½®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šä½¿ç”¨è®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬
./train_local.sh
```

### æ–¹æ³•2ï¼šæ‰‹åŠ¨å‘½ä»¤

```bash
# åŸºç¡€è®­ç»ƒ
python scripts/train.py \
    --model_config config/gpu_model_config.yaml \
    --lora_config config/gpu_lora_config.yaml \
    --device cuda

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
python scripts/train.py \
    --model_config config/gpu_model_config.yaml \
    --lora_config config/gpu_lora_config.yaml \
    --device cuda \
    --learning_rate 1.5e-4 \
    --batch_size 2 \
    --num_epochs 3 \
    --output_dir models/my_checkpoints
```

## ğŸ“Š é…ç½®è¯´æ˜

### RTX 4060 ä¼˜åŒ–é…ç½® (`config/gpu_model_config.yaml`)

- **æ¨¡å‹è·¯å¾„**: `./models` (æœ¬åœ°)
- **æ•°æ®ç±»å‹**: bfloat16 (GPUæ•°å€¼ç¨³å®šæ€§æ›´å¥½)
- **æ‰¹æ¬¡å¤§å°**: 2 (æ¯GPU)
- **æ¢¯åº¦ç´¯ç§¯**: 32æ­¥ (æœ‰æ•ˆæ‰¹æ¬¡ = 2Ã—32 = 64)
- **åºåˆ—é•¿åº¦**: 1024 tokens
- **å­¦ä¹ ç‡**: 1.5e-4 (Qwenæ¨èå€¼)
- **Flash Attention 2**: å¯ç”¨ (åŠ é€Ÿè®­ç»ƒ)
- **é¢„ä¼°æ˜¾å­˜**: ~7.3GB (RTX 4060å®‰å…¨èŒƒå›´)

### LoRA é…ç½® (`config/gpu_lora_config.yaml`)

- **Rank (r)**: 16 (å¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜)
- **Alpha**: 32 (2Ã—r)
- **Dropout**: 0.1
- **ç›®æ ‡æ¨¡å—**: æ‰€æœ‰Qwen2çº¿æ€§å±‚ (7ä¸ªæ¨¡å—)
- **å¯è®­ç»ƒå‚æ•°**: ~4.2M (0.14%æ€»å‚æ•°)

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

- **è®­ç»ƒæ—¶é—´**: 2-3å°æ—¶ (3ä¸ªepoch)
- **æ˜¾å­˜ä½¿ç”¨**: 6-7GB VRAM
- **è®­ç»ƒé€Ÿåº¦**: 30-50 tokens/ç§’
- **æ€§èƒ½æå‡**: 15-25% (æŒ‡ä»¤è·Ÿéšä»»åŠ¡)

## ğŸ” éªŒè¯æ­¥éª¤

### 1. æµ‹è¯•GPUç¯å¢ƒ
```bash
python scripts/test_gpu_env.py
```

### 2. å¿«é€Ÿæ–‡ä»¶æ£€æŸ¥
```bash
python scripts/quick_test.py
```

### 3. å®Œæ•´ç¯å¢ƒæµ‹è¯•ï¼ˆåŒ…å«æ¨¡å‹åŠ è½½ï¼‰
```bash
python scripts/test_local_setup.py
```

## ğŸ“ è®­ç»ƒç›‘æ§

### å®æ—¶ç›‘æ§GPUä½¿ç”¨
```bash
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
```bash
tail -f models/qwen_local_checkpoints/training.log
```

### æ£€æŸ¥æ£€æŸ¥ç‚¹
```bash
ls -lh models/qwen_local_checkpoints/
```

## âš™ï¸ é«˜çº§é€‰é¡¹

### è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœæ˜¾å­˜ä¸è¶³ï¼‰
```bash
# å‡å°æ‰¹æ¬¡å¤§å°åˆ°1
python scripts/train.py \
    --model_config config/gpu_model_config.yaml \
    --lora_config config/gpu_lora_config.yaml \
    --device cuda \
    --batch_size 1
```

### è°ƒæ•´åºåˆ—é•¿åº¦ï¼ˆå¦‚æœOOMï¼‰
```bash
# å‡å°åºåˆ—é•¿åº¦åˆ°512
python scripts/train.py \
    --model_config config/gpu_model_config.yaml \
    --lora_config config/gpu_lora_config.yaml \
    --device cuda \
    --max_length 512
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
```bash
python scripts/train.py \
    --model_config config/gpu_model_config.yaml \
    --lora_config config/gpu_lora_config.yaml \
    --device cuda \
    --resume_from_checkpoint models/qwen_local_checkpoints/checkpoint-500
```

## ğŸ¯ è®­ç»ƒåä½¿ç”¨

### è¿è¡Œæ¨ç†
```bash
python scripts/inference.py \
    --model_path models/qwen_local_checkpoints/final/ \
    --device cuda
```

### è¯„ä¼°æ¨¡å‹
```bash
python scripts/evaluate.py \
    --model_path models/qwen_local_checkpoints/final/ \
    --data_path data/raw/alpaca_data_cleaned.json \
    --device cuda
```

## ğŸ”§ æ•…éšœæ’é™¤

### æ˜¾å­˜ä¸è¶³ (OOM)
- å‡å° `batch_size` åˆ° 1
- å‡å° `max_length` åˆ° 512
- å‡å° LoRA `rank` åˆ° 8

### è®­ç»ƒé€Ÿåº¦æ…¢
- æ£€æŸ¥ Flash Attention 2 æ˜¯å¦å¯ç”¨
- ç¡®ä¿ `dataloader_num_workers` > 0
- æ£€æŸ¥ GPU æ˜¯å¦æ­£ç¡®è¯†åˆ«

### æ¨¡å‹åŠ è½½å¤±è´¥
- ç¡®è®¤æ¨¡å‹æ–‡ä»¶å®Œæ•´: `python scripts/quick_test.py`
- æ£€æŸ¥ `config.json` å’Œ `tokenizer.json` å­˜åœ¨
- éªŒè¯æ¨¡å‹è·¯å¾„é…ç½®æ­£ç¡®

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
finetuning-practice/
â”œâ”€â”€ models/                          # æœ¬åœ°æ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚   â””â”€â”€ model-00002-of-00002.safetensors
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ alpaca_data_cleaned.json # æœ¬åœ°æ•°æ®
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ gpu_model_config.yaml        # RTX 4060æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ gpu_lora_config.yaml         # LoRAé…ç½®
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                     # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ quick_test.py                # å¿«é€Ÿæµ‹è¯•
â”‚   â””â”€â”€ test_local_setup.py          # å®Œæ•´æµ‹è¯•
â””â”€â”€ train_local.sh                   # ä¸€é”®è®­ç»ƒè„šæœ¬
```

## ğŸ‰ å¼€å§‹è®­ç»ƒ

ä¸€åˆ‡å°±ç»ªï¼è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒï¼š

```bash
./train_local.sh
```

æˆ–è€…ç›´æ¥ä½¿ç”¨ Pythonï¼š

```bash
python scripts/train.py \
    --model_config config/gpu_model_config.yaml \
    --lora_config config/gpu_lora_config.yaml \
    --device cuda
```

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
