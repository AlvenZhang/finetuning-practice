#!/usr/bin/env python3
"""
GPU æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æ£€æŸ¥ WSL GPU æ€§èƒ½æ˜¯å¦æ­£å¸¸
"""

import torch
import time
import numpy as np
from datetime import datetime

def test_gpu_availability():
    """æ£€æŸ¥ GPU å¯ç”¨æ€§"""
    print("=" * 60)
    print("ðŸ” GPU å¯ç”¨æ€§æ£€æŸ¥")
    print("=" * 60)

    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼è¯·æ£€æŸ¥ CUDA å®‰è£…")
        return False

    print(f"âœ… CUDA å¯ç”¨")
    print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰ GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"PyTorch CUDA ç‰ˆæœ¬: {torch.cuda.get_arch_list()}")
    return True


def get_gpu_memory_info():
    """èŽ·å– GPU å†…å­˜ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ðŸ’¾ GPU å†…å­˜ä¿¡æ¯")
    print("=" * 60)

    device = torch.cuda.current_device()
    total_memory = torch.cuda.get_device_properties(device).total_memory
    allocated_memory = torch.cuda.memory_allocated(device)
    cached_memory = torch.cuda.memory_reserved(device)

    print(f"æ€»å†…å­˜: {total_memory / 1024**3:.2f} GB")
    print(f"å·²åˆ†é…: {allocated_memory / 1024**3:.2f} GB")
    print(f"å·²ç¼“å­˜: {cached_memory / 1024**3:.2f} GB")
    print(f"å¯ç”¨å†…å­˜: {(total_memory - allocated_memory) / 1024**3:.2f} GB")


def benchmark_matrix_multiplication(size=4096, num_iterations=100):
    """çŸ©é˜µä¹˜æ³•åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 60)
    print(f"âš¡ çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯• ({size}x{size} x {num_iterations} æ¬¡)")
    print("=" * 60)

    device = torch.device("cuda:0")

    # é¢„çƒ­
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    for _ in range(10):
        _ = torch.mm(a, b)

    torch.cuda.synchronize()
    start_time = time.time()

    for _ in range(num_iterations):
        c = torch.mm(a, b)

    torch.cuda.synchronize()
    elapsed_time = time.time() - start_time

    total_ops = 2 * size ** 3 * num_iterations  # 2 * n^3 ä¹˜åŠ æ“ä½œ
    gflops = total_ops / (elapsed_time * 1e9)

    print(f"â±ï¸  æ€»æ—¶é—´: {elapsed_time:.3f} ç§’")
    print(f"ðŸ“Š æ€§èƒ½: {gflops:.2f} GFLOPS")
    print(f"ðŸš€ æ¯æ¬¡è¿­ä»£: {elapsed_time/num_iterations*1000:.2f} ms")

    # æ€§èƒ½å‚è€ƒ
    expected_gflops = {
        "RTX 4060": 15000,  # ç†è®ºå³°å€¼
        "RTX 3060": 13000,
        "GTX 1660": 5000,
    }

    current_gpu = torch.cuda.get_device_name(0)
    print(f"\nðŸ“ˆ æ€§èƒ½è¯„ä¼°:")
    if gflops > 10000:
        print(f"âœ… æ€§èƒ½ä¼˜ç§€ ({gflops:.0f} GFLOPS)")
    elif gflops > 5000:
        print(f"âš ï¸  æ€§èƒ½ä¸€èˆ¬ ({gflops:.0f} GFLOPS)")
    else:
        print(f"âŒ æ€§èƒ½è¾ƒå·® ({gflops:.0f} GFLOPS) - å¯èƒ½å­˜åœ¨é—®é¢˜")

    return gflops


def benchmark_memory_transfer(size=1024*1024*100):  # 100M å…ƒç´ 
    """å†…å­˜ä¼ è¾“é€Ÿåº¦æµ‹è¯•"""
    print("\n" + "=" * 60)
    print(f"ðŸ”„ å†…å­˜ä¼ è¾“é€Ÿåº¦æµ‹è¯• ({size/1e6:.1f}M å…ƒç´ )")
    print("=" * 60)

    device = torch.device("cuda:0")

    # CPU -> GPU
    cpu_tensor = torch.randn(size)
    torch.cuda.synchronize()
    start_time = time.time()

    for _ in range(10):
        gpu_tensor = cpu_tensor.to(device)

    torch.cuda.synchronize()
    cpu_to_gpu_time = (time.time() - start_time) / 10

    # GPU -> CPU
    start_time = time.time()

    for _ in range(10):
        cpu_tensor_back = gpu_tensor.to('cpu')

    torch.cuda.synchronize()
    gpu_to_cpu_time = (time.time() - start_time) / 10

    data_size_gb = size * 4 / 1e9  # float32 = 4 bytes

    print(f"CPU -> GPU: {cpu_to_gpu_time*1000:.2f} ms ({data_size_gb/cpu_to_gpu_time:.2f} GB/s)")
    print(f"GPU -> CPU: {gpu_to_cpu_time*1000:.2f} ms ({data_size_gb/gpu_to_cpu_time:.2f} GB/s)")

    # æ€§èƒ½è¯„ä¼°
    print(f"\nðŸ“ˆ ä¼ è¾“é€Ÿåº¦è¯„ä¼°:")
    if cpu_to_gpu_time < 0.01:  # < 10ms
        print("âœ… ä¼ è¾“é€Ÿåº¦ä¼˜ç§€")
    elif cpu_to_gpu_time < 0.05:  # < 50ms
        print("âš ï¸  ä¼ è¾“é€Ÿåº¦ä¸€èˆ¬")
    else:
        print("âŒ ä¼ è¾“é€Ÿåº¦è¾ƒæ…¢ - å¯èƒ½å­˜åœ¨ WSL/Windows GPU é©±åŠ¨é—®é¢˜")


def benchmark_pytorch_operations():
    """PyTorch å¸¸è§æ“ä½œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("ðŸ”¥ PyTorch æ“ä½œæ€§èƒ½æµ‹è¯•")
    print("=" * 60)

    device = torch.device("cuda:0")
    batch_size = 32
    seq_length = 512
    hidden_dim = 768

    # æ¨¡æ‹Ÿ Transformer å±‚è®¡ç®—
    x = torch.randn(batch_size, seq_length, hidden_dim, device=device)

    # é¢„çƒ­
    for _ in range(10):
        y = torch.matmul(x, x.transpose(-2, -1))

    torch.cuda.synchronize()
    start_time = time.time()

    num_iterations = 100
    for _ in range(num_iterations):
        y = torch.matmul(x, x.transpose(-2, -1))

    torch.cuda.synchronize()
    elapsed_time = time.time() - start_time

    print(f"æ³¨æ„åŠ›æœºåˆ¶è®¡ç®— ({batch_size}x{seq_length}x{hidden_dim}):")
    print(f"â±ï¸  {elapsed_time/num_iterations*1000:.2f} ms/æ¬¡")
    print(f"ðŸš€ {num_iterations/elapsed_time:.1f} æ¬¡/ç§’")


def check_wsl_performance_issues():
    """æ£€æŸ¥ WSL ç‰¹å®šçš„æ€§èƒ½é—®é¢˜"""
    print("\n" + "=" * 60)
    print("ðŸªŸ WSL æ€§èƒ½é—®é¢˜æ£€æŸ¥")
    print("=" * 60)

    import subprocess
    import os

    # æ£€æŸ¥ WSL ç‰ˆæœ¬
    try:
        result = subprocess.run(['wsl', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… WSL ç‰ˆæœ¬ä¿¡æ¯:")
            print(result.stdout)
    except:
        print("âš ï¸  æ— æ³•èŽ·å– WSL ç‰ˆæœ¬ä¿¡æ¯")

    # æ£€æŸ¥æ˜¯å¦åœ¨ WSL2 ä¸­è¿è¡Œ
    try:
        with open('/proc/version', 'r') as f:
            version = f.read()
            if 'microsoft' in version.lower():
                print("âœ… æ­£åœ¨ WSL çŽ¯å¢ƒä¸­è¿è¡Œ")
                if 'wsl2' in version.lower() or '2' in version:
                    print("âœ… WSL2 æ¨¡å¼ (æŽ¨è)")
                else:
                    print("âš ï¸  å¯èƒ½æ˜¯ WSL1ï¼ŒGPU æ”¯æŒæœ‰é™")
    except:
        print("âŒ æ— æ³•ç¡®å®š WSL ç‰ˆæœ¬")

    # æ£€æŸ¥ Windows GPU é©±åŠ¨
    print("\nå»ºè®®åœ¨ Windows ä¸­æ£€æŸ¥:")
    print("1. è¿è¡Œ 'nvidia-smi' æŸ¥çœ‹é©±åŠ¨ç‰ˆæœ¬")
    print("2. è®¿é—® https://www.nvidia.com/Download/index.aspx æ›´æ–°é©±åŠ¨")
    print("3. ç¡®ä¿å®‰è£…äº† WSL2 æ”¯æŒçš„æœ€æ–°é©±åŠ¨")


def compare_with_native_windows():
    """å»ºè®®ä¸ŽåŽŸç”Ÿ Windows æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("ðŸ“Š æ€§èƒ½å¯¹æ¯”å»ºè®®")
    print("=" * 60)

    print("è¦å‡†ç¡®è¯„ä¼° WSL æ€§èƒ½æŸå¤±ï¼Œå»ºè®®:")
    print("1. åœ¨ Windows åŽŸç”ŸçŽ¯å¢ƒä¸­è¿è¡Œç›¸åŒæµ‹è¯•")
    print("2. å¯¹æ¯”ç›¸åŒæ“ä½œçš„æ—¶é—´å·®å¼‚")
    print("3. WSL2 é€šå¸¸æœ‰ 5-15% çš„æ€§èƒ½æŸå¤±æ˜¯æ­£å¸¸çš„")
    print("4. å¦‚æžœæŸå¤±è¶…è¿‡ 20%ï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ðŸš€ å¼€å§‹ GPU æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print(f"â° æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if not test_gpu_availability():
        return

    get_gpu_memory_info()

    # è¿è¡Œå„ç§åŸºå‡†æµ‹è¯•
    try:
        gflops = benchmark_matrix_multiplication()
        benchmark_memory_transfer()
        benchmark_pytorch_operations()
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return

    check_wsl_performance_issues()
    compare_with_native_windows()

    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 60)
    print("\nðŸ’¡ å»ºè®®:")
    print("1. å¦‚æžœæ€§èƒ½æ˜¾è‘—ä½ŽäºŽé¢„æœŸï¼Œè€ƒè™‘æ›´æ–° NVIDIA é©±åŠ¨")
    print("2. ç¡®ä¿ä½¿ç”¨ WSL2 è€Œéž WSL1")
    print("3. æ£€æŸ¥ Windows ç”µæºè®¡åˆ’è®¾ç½®ä¸ºé«˜æ€§èƒ½")
    print("4. å…³é—­åŽå°ç¨‹åºä»¥é‡Šæ”¾ GPU èµ„æº")


if __name__ == "__main__":
    main()