#!/usr/bin/env python3
"""
ä¸»è®­ç»ƒè„šæœ¬
Main training script for LLM fine-tuning with LoRA
"""

import os
import sys
import argparse
import yaml
from pathlib import Path
import logging
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.training.trainer import LoRATrainer
from src.model.model_loader import ModelLoader
from src.data.dataset import create_dataloaders
from src.utils.logging import setup_logging, MemoryLogger

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="LLM LoRAå¾®è°ƒè®­ç»ƒè„šæœ¬")

    # åŸºç¡€å‚æ•°
    parser.add_argument(
        "--model_config",
        type=str,
        default="config/qwen_model_config.yaml",
        help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--lora_config",
        type=str,
        default="config/qwen_lora_config.yaml",
        help="LoRAé…ç½®æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--output_dir",
        type=str,
        default="models/checkpoints",
        help="è¾“å‡ºç›®å½•"
    )

    parser.add_argument(
        "--experiment_name",
        type=str,
        default=None,
        help="å®éªŒåç§°"
    )

    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--train_data",
        type=str,
        default="data/processed/alpaca_train.json",
        help="è®­ç»ƒæ•°æ®è·¯å¾„"
    )

    parser.add_argument(
        "--val_data",
        type=str,
        default="data/processed/alpaca_validation.json",
        help="éªŒè¯æ•°æ®è·¯å¾„"
    )

    # è®­ç»ƒå‚æ•°è¦†ç›–
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=None,
        help="å­¦ä¹ ç‡"
    )

    parser.add_argument(
        "--batch_size",
        type=int,
        default=None,
        help="æ‰¹æ¬¡å¤§å°"
    )

    parser.add_argument(
        "--num_epochs",
        type=int,
        default=None,
        help="è®­ç»ƒè½®æ¬¡"
    )

    parser.add_argument(
        "--max_length",
        type=int,
        default=None,
        help="æœ€å¤§åºåˆ—é•¿åº¦"
    )

    # LoRAå‚æ•°è¦†ç›–
    parser.add_argument(
        "--lora_r",
        type=int,
        default=None,
        help="LoRA rank"
    )

    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=None,
        help="LoRA alpha"
    )

    parser.add_argument(
        "--lora_dropout",
        type=float,
        default=None,
        help="LoRA dropout"
    )

    # å…¶ä»–å‚æ•°
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ"
    )

    parser.add_argument(
        "--no_wandb",
        action="store_true",
        help="ç¦ç”¨Weights & Biasesè·Ÿè¸ª"
    )

    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )

    parser.add_argument(
        "--force_cpu",
        action="store_true",
        help="å¼ºåˆ¶ä½¿ç”¨CPU"
    )

    return parser.parse_args()

def load_and_override_config(config_path: str, overrides: dict) -> dict:
    """åŠ è½½é…ç½®å¹¶åº”ç”¨è¦†ç›–"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # åº”ç”¨è¦†ç›–
    for key, value in overrides.items():
        if value is not None:
            # æ”¯æŒåµŒå¥—é”® (å¦‚ training.learning_rate)
            keys = key.split('.')
            current = config
            for k in keys[:-1]:
                if k not in current:
                    current[k] = {}
                current = current[k]
            current[keys[-1]] = value

    return config

def check_data_files(train_path: str, val_path: str) -> bool:
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    train_exists = Path(train_path).exists()
    val_exists = Path(val_path).exists()

    if not train_exists:
        print(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_path}")
        print("è¯·å…ˆè¿è¡Œ: python data/download_data.py")

    if not val_exists:
        print(f"âŒ éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {val_path}")
        print("è¯·å…ˆè¿è¡Œ: python data/download_data.py")

    return train_exists and val_exists

def setup_device(force_cpu: bool = False) -> str:
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if force_cpu:
        device = "cpu"
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
    elif torch.cuda.is_available():
        device = "cuda"
        gpu_name = torch.cuda.get_device_name()
        print(f"ğŸš€ ä½¿ç”¨GPU: {gpu_name}")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("ğŸ ä½¿ç”¨Apple Silicon MPS")
    else:
        device = "cpu"
        print("ğŸ–¥ï¸  ä½¿ç”¨CPU")

    return device

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    import psutil
    import platform

    print("\\n=== ç³»ç»Ÿä¿¡æ¯ ===")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"CPU: {platform.processor()}")
    print(f"å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")

    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)} ({gpu_memory:.1f} GB)")

    if torch.backends.mps.is_available():
        print("Apple Silicon MPS: å¯ç”¨")

    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    log_level = "DEBUG" if args.debug else "INFO"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    setup_logging(
        log_file=output_dir / "training.log",
        level=log_level
    )

    logger = logging.getLogger(__name__)
    memory_logger = MemoryLogger()

    logger.info("å¼€å§‹LLM LoRAå¾®è°ƒè®­ç»ƒ")

    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()

    # è®¾ç½®è®¾å¤‡
    device = setup_device(args.force_cpu)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_data_files(args.train_data, args.val_data):
        return 1

    try:
        # è®°å½•åˆå§‹å†…å­˜
        memory_logger.log_memory_usage("å¼€å§‹")

        # å‡†å¤‡é…ç½®è¦†ç›–
        config_overrides = {}

        # è®­ç»ƒå‚æ•°è¦†ç›–
        if args.learning_rate is not None:
            config_overrides['training.learning_rate'] = args.learning_rate
        if args.batch_size is not None:
            config_overrides['training.per_device_train_batch_size'] = args.batch_size
            config_overrides['training.per_device_eval_batch_size'] = args.batch_size
        if args.num_epochs is not None:
            config_overrides['training.num_train_epochs'] = args.num_epochs
        if args.max_length is not None:
            config_overrides['data.max_length'] = args.max_length

        # åº”ç”¨é…ç½®è¦†ç›–
        if config_overrides:
            logger.info(f"åº”ç”¨é…ç½®è¦†ç›–: {config_overrides}")

        # LoRAå‚æ•°è¦†ç›–
        lora_overrides = {}
        if args.lora_r is not None:
            lora_overrides['lora.r'] = args.lora_r
        if args.lora_alpha is not None:
            lora_overrides['lora.lora_alpha'] = args.lora_alpha
        if args.lora_dropout is not None:
            lora_overrides['lora.lora_dropout'] = args.lora_dropout

        # ç¦ç”¨wandbï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.no_wandb:
            os.environ["WANDB_DISABLED"] = "true"

        # åˆå§‹åŒ–è®­ç»ƒå™¨
        logger.info("åˆå§‹åŒ–è®­ç»ƒå™¨...")
        trainer = LoRATrainer(
            model_config_path=args.model_config,
            lora_config_path=args.lora_config,
            output_dir=args.output_dir,
            experiment_name=args.experiment_name
        )

        # åº”ç”¨é…ç½®è¦†ç›–
        if config_overrides:
            for key, value in config_overrides.items():
                keys = key.split('.')
                current = trainer.model_config
                for k in keys[:-1]:
                    current = current[k]
                current[keys[-1]] = value

        if lora_overrides:
            for key, value in lora_overrides.items():
                keys = key.split('.')
                current = trainer.lora_config
                for k in keys[:-1]:
                    current = current[k]
                current[keys[-1]] = value

        # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        logger.info("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        trainer.setup_model_and_tokenizer()
        memory_logger.log_memory_usage("æ¨¡å‹åŠ è½½å")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_loader, val_loader = create_dataloaders(
            train_path=args.train_data,
            val_path=args.val_data,
            tokenizer=trainer.tokenizer,
            batch_size=trainer.model_config['training']['per_device_train_batch_size'],
            max_length=trainer.model_config['data']['max_length']
        )

        trainer.setup_data_loaders(train_loader, val_loader)
        memory_logger.log_memory_usage("æ•°æ®åŠ è½½å™¨åˆ›å»ºå")

        # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.resume_from_checkpoint:
            logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume_from_checkpoint}")
            trainer.resume_from_checkpoint(args.resume_from_checkpoint)

        # å¼€å§‹è®­ç»ƒ
        logger.info("å¼€å§‹è®­ç»ƒ...")
        print("\\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ“Š å®éªŒåç§°: {trainer.experiment_name}")

        trainer.train()

        # è®­ç»ƒå®Œæˆ
        logger.info("è®­ç»ƒå®Œæˆï¼")
        print("\\nâœ… è®­ç»ƒå®Œæˆï¼")

        # æœ€ç»ˆå†…å­˜ä½¿ç”¨
        memory_logger.log_memory_usage("è®­ç»ƒå®Œæˆ")

        return 0

    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print("\\nâ¸ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return 0

    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
        print(f"\\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)