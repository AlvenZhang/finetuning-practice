#!/usr/bin/env python3
"""
æµ‹è¯• Qwen æ¨¡å‹åŠ è½½
Test Qwen model loading
"""

import sys
import os
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.model.model_loader import ModelLoader

def test_qwen_loading():
    """æµ‹è¯•Qwenæ¨¡å‹åŠ è½½"""
    print("=== æµ‹è¯• Qwen æ¨¡å‹åŠ è½½ ===")

    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = "./models/base/qwen2.5-3b-instruct"
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False

        print(f"âœ… æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path}")

        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        required_files = [
            "config.json",
            "tokenizer.json",
            "tokenizer_config.json",
            "model-00001-of-00002.safetensors",
            "model-00002-of-00002.safetensors"
        ]

        for file in required_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                print(f"âœ… {file} å­˜åœ¨")
            else:
                print(f"âŒ {file} ä¸å­˜åœ¨")
                return False

        # åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨
        config_path = "./config/qwen_model_config.yaml"
        loader = ModelLoader(config_path)

        print("\\n=== åŠ è½½åˆ†è¯å™¨ ===")
        tokenizer = loader.load_tokenizer(model_path)
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        print(f"   ç‰¹æ®Štoken: bos={tokenizer.bos_token}, eos={tokenizer.eos_token}, pad={tokenizer.pad_token}")

        # æµ‹è¯•åˆ†è¯
        test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼Hello, World!"
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)
        print(f"   æµ‹è¯•æ–‡æœ¬: {test_text}")
        print(f"   ç¼–ç ç»“æœ: {tokens[:10]}... (æ˜¾ç¤ºå‰10ä¸ª)")
        print(f"   è§£ç ç»“æœ: {decoded}")

        print("\\n=== åŠ è½½æ¨¡å‹ ===")
        print("âš ï¸  æ³¨æ„: åŠ è½½3Bæ¨¡å‹å¯èƒ½éœ€è¦1-2åˆ†é’Ÿæ—¶é—´...")

        # å°è¯•åŠ è½½æ¨¡å‹ï¼ˆå¯èƒ½ä¼šå› ä¸ºå†…å­˜ä¸è¶³è€Œå¤±è´¥ï¼‰
        try:
            model = loader.load_model(model_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            total_params = sum(p.numel() for p in model.parameters())
            print(f"   æ€»å‚æ•°é‡: {total_params:,}")
            print(f"   æ¨¡å‹ç±»å‹: {type(model).__name__}")
            print(f"   è®¾å¤‡: {next(model.parameters()).device}")

            # æµ‹è¯•ç®€å•æ¨ç†
            print("\\n=== æµ‹è¯•æ¨ç† ===")
            model.eval()
            with torch.no_grad():
                input_ids = tokenizer.encode("ä½ å¥½", return_tensors="pt")
                if torch.backends.mps.is_available():
                    input_ids = input_ids.to("mps")
                    model = model.to("mps")

                outputs = model(input_ids)
                print("âœ… æ¨ç†æµ‹è¯•æˆåŠŸ")
                print(f"   è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸ºå†…å­˜ä¸è¶³ã€‚Qwen 3Béœ€è¦çº¦6-8GBå†…å­˜ã€‚")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„ç¨‹åº")
            print("   2. ä½¿ç”¨é‡åŒ–åŠ è½½ (load_in_4bit=True)")
            print("   3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
            return False

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("Qwen æ¨¡å‹åŠ è½½æµ‹è¯•")
    print("=" * 50)

    # æ£€æŸ¥ç¯å¢ƒ
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"MPSå¯ç”¨: {torch.backends.mps.is_available()}")

    # æ£€æŸ¥å†…å­˜
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("ğŸ Apple Silicon (MPS) ç¯å¢ƒ")
    elif torch.cuda.is_available():
        print(f"ğŸš€ CUDAç¯å¢ƒ: {torch.cuda.get_device_name()}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("ğŸ’» CPUç¯å¢ƒ")

    print("\\n" + "=" * 50)

    # è¿è¡Œæµ‹è¯•
    success = test_qwen_loading()

    if success:
        print("\\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œè®­ç»ƒ: python scripts/train.py --model_config config/qwen_model_config.yaml --lora_config config/qwen_lora_config.yaml")
        print("2. ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("3. å¦‚æœå†…å­˜ä¸è¶³ï¼Œè€ƒè™‘è°ƒæ•´æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨é‡åŒ–")
    else:
        print("\\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()