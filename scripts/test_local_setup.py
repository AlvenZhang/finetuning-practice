#!/usr/bin/env python3
"""
æµ‹è¯•æœ¬åœ°æ¨¡å‹å’Œæ•°æ®åŠ è½½
Test local model and data loading
"""

import sys
from pathlib import Path
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.model.model_loader import ModelLoader
import json

def test_local_model():
    """æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½"""
    print("=== æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½ ===\n")

    try:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹
        loader = ModelLoader("config/gpu_model_config.yaml")

        print("ğŸ“‚ åŠ è½½é…ç½®æ–‡ä»¶: config/gpu_model_config.yaml")
        print(f"ğŸ“¦ æ¨¡å‹è·¯å¾„: {loader.model_config['model']['name']}")
        print(f"ğŸ“ Tokenizerè·¯å¾„: {loader.model_config['tokenizer']['name']}\n")

        # åŠ è½½tokenizer
        print("ğŸ”¤ åŠ è½½tokenizer...")
        tokenizer = loader.load_tokenizer()
        print(f"âœ… TokenizeråŠ è½½æˆåŠŸï¼")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        print(f"   PAD token: {tokenizer.pad_token}")
        print(f"   EOS token: {tokenizer.eos_token}\n")

        # åŠ è½½æ¨¡å‹
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")
        model = loader.load_model()
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"   æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"   è®¾å¤‡: {next(model.parameters()).device}")
        print(f"   æ•°æ®ç±»å‹: {next(model.parameters()).dtype}\n")

        # æ˜¾å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            mem_allocated = torch.cuda.memory_allocated() / 1024**3
            mem_reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ’¾ GPUæ˜¾å­˜ä½¿ç”¨:")
            print(f"   å·²åˆ†é…: {mem_allocated:.2f} GB")
            print(f"   å·²é¢„ç•™: {mem_reserved:.2f} GB")
            print(f"   æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\n")

        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_local_data():
    """æµ‹è¯•æœ¬åœ°æ•°æ®åŠ è½½"""
    print("\n=== æµ‹è¯•æœ¬åœ°æ•°æ®åŠ è½½ ===\n")

    data_path = Path("data/raw/alpaca_data_cleaned.json")

    if not data_path.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False

    try:
        print(f"ğŸ“‚ è¯»å–æ•°æ®æ–‡ä»¶: {data_path}")
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
        print(f"   æ•°æ®ç±»å‹: {type(data)}")

        if isinstance(data, list):
            print(f"   æ ·æœ¬æ•°é‡: {len(data)}")
            if len(data) > 0:
                print(f"   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®: {list(data[0].keys())}")
                print(f"\n   ç¤ºä¾‹æ ·æœ¬:")
                print(f"   æŒ‡ä»¤: {data[0].get('instruction', 'N/A')[:100]}...")
                if 'input' in data[0]:
                    print(f"   è¾“å…¥: {data[0]['input'][:50] if data[0]['input'] else '(ç©º)'}...")
                print(f"   è¾“å‡º: {data[0].get('output', 'N/A')[:100]}...")
        elif isinstance(data, dict):
            print(f"   æ•°æ®é”®: {list(data.keys())}")

        print(f"\n   æ–‡ä»¶å¤§å°: {data_path.stat().st_size / 1024**2:.1f} MB\n")

        return True

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æœ¬åœ°ç¯å¢ƒ...\n")
    print("=" * 60)

    # æµ‹è¯•GPU
    print("\n=== GPUä¿¡æ¯ ===\n")
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨")
        print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜å¤§å°: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}\n")
    else:
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\n")

    # æµ‹è¯•æ¨¡å‹
    model_ok = test_local_model()

    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("\nğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜\n")

    # æµ‹è¯•æ•°æ®
    data_ok = test_local_data()

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   {'âœ…' if model_ok else 'âŒ'} æ¨¡å‹åŠ è½½")
    print(f"   {'âœ…' if data_ok else 'âŒ'} æ•°æ®åŠ è½½")

    if model_ok and data_ok:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   è¿è¡Œè®­ç»ƒ: python scripts/train.py --model_config config/gpu_model_config.yaml --lora_config config/gpu_lora_config.yaml --device cuda")
        return 0
    else:
        print("\nâš ï¸  è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯å¹¶ä¿®å¤åå†è®­ç»ƒ")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
