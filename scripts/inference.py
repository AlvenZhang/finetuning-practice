#!/usr/bin/env python3
"""
æ¨ç†è„šæœ¬
Inference script for fine-tuned models
"""

import sys
import argparse
import torch
from pathlib import Path
import json
import yaml
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.model.model_loader import load_model_for_inference
from src.utils.logging import setup_logging

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="LLMæ¨ç†è„šæœ¬")

    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2.5-3B-Instruct",
        help="åŸºç¡€æ¨¡å‹åç§°"
    )

    parser.add_argument(
        "--lora_path",
        type=str,
        default=None,
        help="LoRAæƒé‡è·¯å¾„"
    )

    parser.add_argument(
        "--config_file",
        type=str,
        default="config/model_config.yaml",
        help="æ¨¡å‹é…ç½®æ–‡ä»¶"
    )

    # æ¨ç†å‚æ•°
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=256,
        help="æœ€å¤§ç”Ÿæˆtokenæ•°"
    )

    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="æ¸©åº¦å‚æ•°"
    )

    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="Top-pé‡‡æ ·"
    )

    parser.add_argument(
        "--top_k",
        type=int,
        default=50,
        help="Top-ké‡‡æ ·"
    )

    parser.add_argument(
        "--do_sample",
        action="store_true",
        help="å¯ç”¨é‡‡æ ·"
    )

    parser.add_argument(
        "--repetition_penalty",
        type=float,
        default=1.1,
        help="é‡å¤æƒ©ç½š"
    )

    # è¾“å…¥è¾“å‡º
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="å•ä¸ªæ¨ç†æç¤º"
    )

    parser.add_argument(
        "--input_file",
        type=str,
        default=None,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--output_file",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--interactive",
        action="store_true",
        help="äº¤äº’å¼æ¨¡å¼"
    )

    # å…¶ä»–å‚æ•°
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="æ‰¹å¤„ç†å¤§å°"
    )

    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¡ç®—è®¾å¤‡"
    )

    return parser.parse_args()

class InferenceEngine:
    """æ¨ç†å¼•æ“"""

    def __init__(
        self,
        model_name: str,
        lora_path: Optional[str] = None,
        config_file: Optional[str] = None,
        device: str = "auto",
        **generation_kwargs
    ):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“

        Args:
            model_name: åŸºç¡€æ¨¡å‹åç§°
            lora_path: LoRAæƒé‡è·¯å¾„
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            **generation_kwargs: ç”Ÿæˆå‚æ•°
        """
        self.model_name = model_name
        self.lora_path = lora_path
        self.config_file = config_file
        self.device = self._setup_device(device)
        self.generation_kwargs = generation_kwargs

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        if lora_path:
            print(f"ğŸ”§ åŠ è½½LoRAæƒé‡: {lora_path}")

        self.model, self.tokenizer = load_model_for_inference(
            model_name=model_name,
            peft_path=lora_path,
            config_path=config_file
        )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device != "auto":
            self.model = self.model.to(self.device)

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.model.device}")

        # è®¾ç½®é»˜è®¤æç¤ºæ¨¡æ¿
        self.prompt_template = "### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n"

    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device

    def format_prompt(self, instruction: str, input_text: str = "") -> str:
        """æ ¼å¼åŒ–æç¤º"""
        return self.prompt_template.format(
            instruction=instruction,
            input=input_text if input_text else ""
        )

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        do_sample: bool = True,
        repetition_penalty: float = 1.1,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·
            top_k: Top-ké‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·
            repetition_penalty: é‡å¤æƒ©ç½š
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            # åˆ†è¯
            inputs = self.tokenizer.encode(
                prompt,
                return_tensors="pt",
                add_special_tokens=True
            )

            # ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device != "auto":
                inputs = inputs.to(self.device)

            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    do_sample=do_sample,
                    repetition_penalty=repetition_penalty,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    **kwargs
                )

            # è§£ç 
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.shape[1]:],  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                skip_special_tokens=True
            )

            return generated_text.strip()

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def generate_batch(
        self,
        prompts: List[str],
        batch_size: int = 1,
        **generation_kwargs
    ) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆ"""
        results = []

        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i + batch_size]
            batch_results = []

            for prompt in batch_prompts:
                result = self.generate(prompt, **generation_kwargs)
                batch_results.append(result)

            results.extend(batch_results)

            # æ˜¾ç¤ºè¿›åº¦
            progress = min(i + batch_size, len(prompts))
            print(f"ğŸ“Š è¿›åº¦: {progress}/{len(prompts)}")

        return results

def interactive_mode(engine: InferenceEngine):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\\nğŸ¯ äº¤äº’å¼æ¨ç†æ¨¡å¼")
    print("è¾“å…¥æŒ‡ä»¤ï¼ŒæŒ‰å›è½¦ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("-" * 50)

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            instruction = input("\\nğŸ’¬ æŒ‡ä»¤: ").strip()

            if instruction.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not instruction:
                continue

            # å¯é€‰çš„è¾“å…¥å†…å®¹
            input_text = input("ğŸ“ è¾“å…¥å†…å®¹ (å¯é€‰): ").strip()

            # æ ¼å¼åŒ–æç¤º
            prompt = engine.format_prompt(instruction, input_text)

            # ç”Ÿæˆå›å¤
            print("\\nğŸ¤– ç”Ÿæˆä¸­...")
            response = engine.generate(prompt, **engine.generation_kwargs)

            # æ˜¾ç¤ºç»“æœ
            print("\\nâœ¨ å›å¤:")
            print(response)
            print("-" * 50)

        except KeyboardInterrupt:
            print("\\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

def load_prompts_from_file(file_path: str) -> List[Dict[str, str]]:
    """ä»æ–‡ä»¶åŠ è½½æç¤º"""
    file_path = Path(file_path)

    if file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    elif file_path.suffix in ['.yaml', '.yml']:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
    elif file_path.suffix == '.txt':
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        data = [{"instruction": line.strip()} for line in lines if line.strip()]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

    return data

def save_results(results: List[Dict], output_file: str):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    output_path = Path(output_file)

    if output_path.suffix == '.json':
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    elif output_path.suffix in ['.yaml', '.yml']:
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(results, f, default_flow_style=False, allow_unicode=True)
    else:
        # é»˜è®¤ä¿å­˜ä¸ºJSON
        output_path = output_path.with_suffix('.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    # è®¾ç½®æ—¥å¿—
    setup_logging(level="INFO")

    print("ğŸš€ LLMæ¨ç†å·¥å…·")
    print("=" * 50)

    try:
        # å‡†å¤‡ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            'max_new_tokens': args.max_new_tokens,
            'temperature': args.temperature,
            'top_p': args.top_p,
            'top_k': args.top_k,
            'do_sample': args.do_sample,
            'repetition_penalty': args.repetition_penalty
        }

        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        engine = InferenceEngine(
            model_name=args.model_name,
            lora_path=args.lora_path,
            config_file=args.config_file,
            device=args.device,
            **generation_kwargs
        )

        # äº¤äº’å¼æ¨¡å¼
        if args.interactive:
            interactive_mode(engine)
            return 0

        # å•ä¸ªæç¤ºæ¨ç†
        if args.prompt:
            print(f"\\nğŸ’¬ æç¤º: {args.prompt}")
            prompt = engine.format_prompt(args.prompt)
            response = engine.generate(prompt)
            print(f"\\nâœ¨ å›å¤:\\n{response}")
            return 0

        # æ–‡ä»¶æ‰¹å¤„ç†
        if args.input_file:
            print(f"\\nğŸ“ åŠ è½½è¾“å…¥æ–‡ä»¶: {args.input_file}")
            prompts_data = load_prompts_from_file(args.input_file)

            results = []
            for i, item in enumerate(prompts_data):
                instruction = item.get('instruction', '')
                input_text = item.get('input', '')

                print(f"\\nå¤„ç† {i+1}/{len(prompts_data)}: {instruction[:50]}...")

                prompt = engine.format_prompt(instruction, input_text)
                response = engine.generate(prompt)

                result = {
                    'instruction': instruction,
                    'input': input_text,
                    'response': response
                }
                results.append(result)

            # ä¿å­˜ç»“æœ
            if args.output_file:
                save_results(results, args.output_file)
            else:
                # æ‰“å°ç»“æœ
                for i, result in enumerate(results):
                    print(f"\\n=== ç»“æœ {i+1} ===")
                    print(f"æŒ‡ä»¤: {result['instruction']}")
                    if result['input']:
                        print(f"è¾“å…¥: {result['input']}")
                    print(f"å›å¤: {result['response']}")

            return 0

        # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•è¾“å…¥ï¼Œæ˜¾ç¤ºå¸®åŠ©
        print("âŒ è¯·æŒ‡å®šè¾“å…¥æ–¹å¼:")
        print("  --prompt 'æŒ‡ä»¤'        : å•ä¸ªæ¨ç†")
        print("  --input_file æ–‡ä»¶è·¯å¾„   : æ‰¹é‡æ¨ç†")
        print("  --interactive         : äº¤äº’å¼æ¨¡å¼")
        return 1

    except KeyboardInterrupt:
        print("\\nğŸ‘‹ æ¨ç†è¢«ç”¨æˆ·ä¸­æ–­")
        return 0
    except Exception as e:
        print(f"\\nâŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)