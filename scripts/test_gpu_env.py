#!/usr/bin/env python3
"""
GPUç¯å¢ƒæ£€æµ‹è„šæœ¬
GPU Environment validation script
"""

import torch
import sys
import subprocess
import logging
from pathlib import Path
import importlib.util

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("=" * 50)
    print("ğŸ Pythonç‰ˆæœ¬æ£€æŸ¥")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")

    if sys.version_info >= (3, 8):
        print("âœ… Pythonç‰ˆæœ¬æ»¡è¶³è¦æ±‚ (>= 3.8)")
        return True
    else:
        print("âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦ >= 3.8")
        return False

def check_cuda_availability():
    """æ£€æŸ¥CUDAå¯ç”¨æ€§"""
    print("\n" + "=" * 50)
    print("ğŸš€ CUDAç¯å¢ƒæ£€æŸ¥")

    # æ£€æŸ¥PyTorch CUDAæ”¯æŒ
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")

        return True
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜"""
    print("\n" + "=" * 50)
    print("ğŸ’¾ GPUå†…å­˜æ£€æŸ¥")

    if not torch.cuda.is_available():
        print("âŒ æ— æ³•æ£€æŸ¥GPUå†…å­˜ï¼šCUDAä¸å¯ç”¨")
        return False

    try:
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            free = total_memory - reserved

            print(f"GPU {i}:")
            print(f"  æ€»æ˜¾å­˜: {total_memory:.1f}GB")
            print(f"  å·²åˆ†é…: {allocated:.1f}GB")
            print(f"  å·²é¢„ç•™: {reserved:.1f}GB")
            print(f"  å¯ç”¨: {free:.1f}GB")

            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è®­ç»ƒè¦æ±‚
            if total_memory >= 8.0:
                print(f"  âœ… æ˜¾å­˜å……è¶³ (>= 8GB)")
            else:
                print(f"  âš ï¸  æ˜¾å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®® >= 8GB")

        return True
    except Exception as e:
        print(f"âŒ æ£€æŸ¥GPUå†…å­˜æ—¶å‡ºé”™: {e}")
        return False

def test_gpu_operations():
    """æµ‹è¯•GPUåŸºæœ¬æ“ä½œ"""
    print("\n" + "=" * 50)
    print("ğŸ§ª GPUæ“ä½œæµ‹è¯•")

    if not torch.cuda.is_available():
        print("âŒ è·³è¿‡GPUæµ‹è¯•ï¼šCUDAä¸å¯ç”¨")
        return False

    try:
        # æµ‹è¯•å¼ é‡æ“ä½œ
        device = torch.device("cuda")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")

        # åˆ›å»ºæµ‹è¯•å¼ é‡
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)

        # çŸ©é˜µä¹˜æ³•æµ‹è¯•
        import time
        start_time = time.time()
        z = torch.mm(x, y)
        torch.cuda.synchronize()  # ç­‰å¾…GPUæ“ä½œå®Œæˆ
        end_time = time.time()

        print(f"çŸ©é˜µä¹˜æ³•æµ‹è¯•: âœ… å®Œæˆ ({end_time - start_time:.3f}s)")

        # æµ‹è¯•æ··åˆç²¾åº¦
        with torch.cuda.amp.autocast():
            z_amp = torch.mm(x.half(), y.half())
        print("æ··åˆç²¾åº¦æµ‹è¯•: âœ… å®Œæˆ")

        # æ¸…ç†æ˜¾å­˜
        del x, y, z, z_amp
        torch.cuda.empty_cache()
        print("æ˜¾å­˜æ¸…ç†: âœ… å®Œæˆ")

        return True

    except Exception as e:
        print(f"âŒ GPUæ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

def check_required_packages():
    """æ£€æŸ¥å¿…éœ€åŒ…"""
    print("\n" + "=" * 50)
    print("ğŸ“¦ ä¾èµ–åŒ…æ£€æŸ¥")

    required_packages = [
        'torch',
        'transformers',
        'peft',
        'datasets',
        'accelerate',
        'bitsandbytes',
    ]

    optional_packages = [
        'flash_attn',
        'deepspeed',
        'xformers',
    ]

    all_good = True

    # æ£€æŸ¥å¿…éœ€åŒ…
    for package in required_packages:
        try:
            if package == 'flash_attn':
                import flash_attn
                version = flash_attn.__version__
            else:
                module = importlib.import_module(package)
                version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {package}: {version}")
        except ImportError:
            print(f"âŒ {package}: æœªå®‰è£…")
            all_good = False

    # æ£€æŸ¥å¯é€‰åŒ…
    print("\nå¯é€‰åŒ…:")
    for package in optional_packages:
        try:
            if package == 'flash_attn':
                import flash_attn
                version = flash_attn.__version__
            else:
                module = importlib.import_module(package)
                version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {package}: {version}")
        except ImportError:
            print(f"âš ï¸  {package}: æœªå®‰è£… (å¯é€‰)")

    return all_good

def check_nvidia_smi():
    """æ£€æŸ¥nvidia-smi"""
    print("\n" + "=" * 50)
    print("ğŸ”§ NVIDIAé©±åŠ¨æ£€æŸ¥")

    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smiå¯ç”¨")
            # æå–å…³é”®ä¿¡æ¯
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version' in line:
                    print(f"é©±åŠ¨ç‰ˆæœ¬: {line.split('Driver Version: ')[1].split()[0]}")
                if 'CUDA Version' in line:
                    print(f"CUDAç‰ˆæœ¬: {line.split('CUDA Version: ')[1].split()[0]}")
            return True
        else:
            print("âŒ nvidia-smiä¸å¯ç”¨")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smiæœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥NVIDIAé©±åŠ¨å®‰è£…")
        return False

def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 50)
    print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")

    if not torch.cuda.is_available():
        print("âŒ è·³è¿‡æ€§èƒ½æµ‹è¯•ï¼šCUDAä¸å¯ç”¨")
        return False

    try:
        device = torch.device("cuda")

        # æµ‹è¯•ä¸åŒç²¾åº¦çš„æ€§èƒ½
        sizes = [512, 1024, 2048]
        dtypes = [torch.float32, torch.float16, torch.bfloat16]

        print("çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•:")
        print("å¤§å°\t\tFP32\t\tFP16\t\tBF16")
        print("-" * 50)

        for size in sizes:
            times = []
            for dtype in dtypes:
                if dtype == torch.bfloat16 and not torch.cuda.is_bf16_supported():
                    times.append("ä¸æ”¯æŒ")
                    continue

                x = torch.randn(size, size, device=device, dtype=dtype)
                y = torch.randn(size, size, device=device, dtype=dtype)

                # é¢„çƒ­
                for _ in range(5):
                    _ = torch.mm(x, y)
                torch.cuda.synchronize()

                # è®¡æ—¶
                import time
                start_time = time.time()
                for _ in range(10):
                    _ = torch.mm(x, y)
                torch.cuda.synchronize()
                end_time = time.time()

                avg_time = (end_time - start_time) / 10 * 1000  # ms
                times.append(f"{avg_time:.1f}ms")

                del x, y
                torch.cuda.empty_cache()

            print(f"{size}x{size}\t\t{times[0]}\t\t{times[1]}\t\t{times[2]}")

        return True

    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GPUè®­ç»ƒç¯å¢ƒæ£€æµ‹å¼€å§‹")
    print("=" * 50)

    checks = [
        ("Pythonç‰ˆæœ¬", check_python_version),
        ("NVIDIAé©±åŠ¨", check_nvidia_smi),
        ("CUDAç¯å¢ƒ", check_cuda_availability),
        ("GPUå†…å­˜", check_gpu_memory),
        ("ä¾èµ–åŒ…", check_required_packages),
        ("GPUæ“ä½œ", test_gpu_operations),
        ("æ€§èƒ½åŸºå‡†", performance_benchmark),
    ]

    results = {}
    for name, check_func in checks:
        try:
            results[name] = check_func()
        except Exception as e:
            print(f"âŒ {name}æ£€æŸ¥æ—¶å‡ºé”™: {e}")
            results[name] = False

    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š æ£€æŸ¥ç»“æœæ€»ç»“")
    print("=" * 50)

    for name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")

    all_passed = all(results.values())
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼GPUè®­ç»ƒç¯å¢ƒå°±ç»ª")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°è¯¦ç»†ä¿¡æ¯")
        print("\nå»ºè®®:")
        if not results.get("CUDAç¯å¢ƒ", True):
            print("- å®‰è£…CUDAå·¥å…·åŒ…å’Œå…¼å®¹çš„PyTorchç‰ˆæœ¬")
        if not results.get("ä¾èµ–åŒ…", True):
            print("- è¿è¡Œ: pip install -r requirements-gpu.txt")
        if not results.get("GPUå†…å­˜", True):
            print("- è€ƒè™‘ä½¿ç”¨é‡åŒ–æˆ–å‡å°‘batch size")

    return all_passed

if __name__ == "__main__":
    main()