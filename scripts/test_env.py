#!/usr/bin/env python3
"""
ç¯å¢ƒæµ‹è¯•è„šæœ¬
Test environment setup for LLM fine-tuning
"""

import sys
import platform
import subprocess
from pathlib import Path

def test_python_version():
    """æµ‹è¯•Pythonç‰ˆæœ¬"""
    print("ğŸ Pythonç‰ˆæœ¬æ£€æŸ¥")
    version = sys.version_info
    print(f"   ç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")

    if version.major >= 3 and version.minor >= 8:
        print("   âœ… Pythonç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 3.8)")
        return True
    else:
        print("   âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦ >= 3.8")
        return False

def test_system_info():
    """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
    print("\\nğŸ’» ç³»ç»Ÿä¿¡æ¯")
    print(f"   æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"   æ¶æ„: {platform.machine()}")
    print(f"   å¤„ç†å™¨: {platform.processor()}")

    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"   å†…å­˜: {memory.total / (1024**3):.1f} GB")
        print(f"   å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
    except ImportError:
        print("   âš ï¸  æ— æ³•è·å–å†…å­˜ä¿¡æ¯ (psutilæœªå®‰è£…)")

def test_pytorch():
    """æµ‹è¯•PyTorchå®‰è£…"""
    print("\\nğŸ”¥ PyTorchæ£€æŸ¥")
    try:
        import torch
        print(f"   ç‰ˆæœ¬: {torch.__version__}")

        # æ£€æŸ¥è®¾å¤‡æ”¯æŒ
        if torch.cuda.is_available():
            print("   âœ… CUDAå¯ç”¨")
            print(f"      CUDAç‰ˆæœ¬: {torch.version.cuda}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                print(f"      GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        elif torch.backends.mps.is_available():
            print("   âœ… Apple Silicon MPSå¯ç”¨")
            print("      é€‚ç”¨äºMç³»åˆ—èŠ¯ç‰‡")
        else:
            print("   âš ï¸  ä»…CPUå¯ç”¨")

        return True
    except ImportError:
        print("   âŒ PyTorchæœªå®‰è£…")
        return False

def test_transformers():
    """æµ‹è¯•Transformersåº“"""
    print("\\nğŸ¤— Transformersæ£€æŸ¥")
    try:
        import transformers
        print(f"   ç‰ˆæœ¬: {transformers.__version__}")

        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        from transformers import AutoTokenizer
        print("   âœ… åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        return True
    except ImportError:
        print("   âŒ Transformersæœªå®‰è£…")
        return False

def test_peft():
    """æµ‹è¯•PEFTåº“"""
    print("\\nğŸ”§ PEFTæ£€æŸ¥")
    try:
        import peft
        print(f"   ç‰ˆæœ¬: {peft.__version__}")
        print("   âœ… LoRAæ”¯æŒå¯ç”¨")
        return True
    except ImportError:
        print("   âŒ PEFTæœªå®‰è£…")
        return False

def test_datasets():
    """æµ‹è¯•Datasetsåº“"""
    print("\\nğŸ“Š Datasetsæ£€æŸ¥")
    try:
        import datasets
        print(f"   ç‰ˆæœ¬: {datasets.__version__}")
        print("   âœ… æ•°æ®é›†å¤„ç†å¯ç”¨")
        return True
    except ImportError:
        print("   âŒ Datasetsæœªå®‰è£…")
        return False

def test_mlx():
    """æµ‹è¯•MLXåº“ (Apple Silicon)"""
    print("\\nğŸ MLXæ£€æŸ¥ (Apple Siliconä¼˜åŒ–)")
    try:
        import mlx
        import mlx.core as mx
        print(f"   MLXç‰ˆæœ¬: {mx.__version__}")

        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        x = mx.array([1, 2, 3])
        y = mx.array([4, 5, 6])
        z = x + y
        print("   âœ… MLXåŸºæœ¬æ“ä½œæ­£å¸¸")

        try:
            import mlx_lm
            print("   âœ… MLX-LMå¯ç”¨")
        except ImportError:
            print("   âš ï¸  MLX-LMæœªå®‰è£…")

        return True
    except ImportError:
        print("   âŒ MLXæœªå®‰è£… (ä»…Apple Siliconéœ€è¦)")
        return False

def test_optional_packages():
    """æµ‹è¯•å¯é€‰åŒ…"""
    print("\\nğŸ“¦ å¯é€‰åŒ…æ£€æŸ¥")

    packages = {
        'wandb': 'Weights & Biaseså®éªŒè·Ÿè¸ª',
        'matplotlib': 'å›¾è¡¨ç»˜åˆ¶',
        'seaborn': 'æ•°æ®å¯è§†åŒ–',
        'jupyter': 'Jupyter Notebook',
        'evaluate': 'è¯„ä¼°æŒ‡æ ‡',
        'rouge_score': 'ROUGEè¯„ä¼°',
        'sacrebleu': 'BLEUè¯„ä¼°'
    }

    results = {}
    for package, description in packages.items():
        try:
            __import__(package)
            print(f"   âœ… {package}: {description}")
            results[package] = True
        except ImportError:
            print(f"   âš ï¸  {package}: {description} (æœªå®‰è£…)")
            results[package] = False

    return results

def test_huggingface_login():
    """æµ‹è¯•Hugging Faceç™»å½•"""
    print("\\nğŸ¤— Hugging Faceç™»å½•æ£€æŸ¥")
    try:
        from huggingface_hub import HfApi
        api = HfApi()
        user_info = api.whoami()
        print(f"   âœ… å·²ç™»å½•: {user_info['name']}")
        return True
    except Exception:
        print("   âš ï¸  æœªç™»å½•æˆ–tokenæ— æ•ˆ")
        print("      è¿è¡Œ: huggingface-cli login")
        return False

def test_project_structure():
    """æµ‹è¯•é¡¹ç›®ç»“æ„"""
    print("\\nğŸ“ é¡¹ç›®ç»“æ„æ£€æŸ¥")

    required_dirs = [
        'config', 'data/raw', 'data/processed', 'src',
        'scripts', 'models', 'experiments'
    ]

    required_files = [
        'requirements.txt', 'README.md',
        'config/model_config.yaml',
        'config/lora_config.yaml',
        'config/eval_config.yaml'
    ]

    all_good = True

    for dir_path in required_dirs:
        if Path(dir_path).exists():
            print(f"   âœ… {dir_path}/")
        else:
            print(f"   âŒ {dir_path}/ (ç¼ºå¤±)")
            all_good = False

    for file_path in required_files:
        if Path(file_path).exists():
            print(f"   âœ… {file_path}")
        else:
            print(f"   âŒ {file_path} (ç¼ºå¤±)")
            all_good = False

    return all_good

def run_installation_test():
    """è¿è¡Œå®‰è£…æµ‹è¯•"""
    print("\\nğŸ§ª è¿è¡Œç®€å•å®‰è£…æµ‹è¯•")

    try:
        # æµ‹è¯•åŸºæœ¬å¯¼å…¥
        print("   æµ‹è¯•åŸºæœ¬å¯¼å…¥...")
        import torch
        import transformers
        import peft
        import datasets

        # æµ‹è¯•åˆ†è¯å™¨åŠ è½½
        print("   æµ‹è¯•åˆ†è¯å™¨åŠ è½½...")
        from transformers import AutoTokenizer

        # ä½¿ç”¨ä¸€ä¸ªå°æ¨¡å‹æµ‹è¯•
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        test_text = "Hello, world!"
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)

        print("   âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"   âŒ å®‰è£…æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LLMå¾®è°ƒç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)

    results = {}

    # åŸºæœ¬æ£€æŸ¥
    results['python'] = test_python_version()
    test_system_info()
    results['pytorch'] = test_pytorch()
    results['transformers'] = test_transformers()
    results['peft'] = test_peft()
    results['datasets'] = test_datasets()

    # Apple Siliconç‰¹å®š
    if platform.machine() in ['arm64', 'aarch64']:
        results['mlx'] = test_mlx()
    else:
        results['mlx'] = True  # éApple Siliconä¸éœ€è¦

    # å¯é€‰åŒ…
    optional_results = test_optional_packages()

    # å…¶ä»–æ£€æŸ¥
    results['hf_login'] = test_huggingface_login()
    results['project_structure'] = test_project_structure()
    results['installation_test'] = run_installation_test()

    # æ€»ç»“
    print("\\n" + "=" * 60)
    print("ğŸ“‹ æ£€æŸ¥æ€»ç»“")

    required_checks = ['python', 'pytorch', 'transformers', 'peft', 'datasets', 'mlx']
    passed = sum(results.get(check, False) for check in required_checks)
    total = len(required_checks)

    print(f"\\nå¿…éœ€ç»„ä»¶: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("âœ… ç¯å¢ƒé…ç½®å®Œæ•´ï¼Œå¯ä»¥å¼€å§‹å¾®è°ƒï¼")

        # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
        print("\\nğŸ¯ ä¸‹ä¸€æ­¥:")
        print("1. ä¸‹è½½æ•°æ®: python data/download_data.py")
        print("2. ä¸‹è½½æ¨¡å‹: python scripts/download_model.py")
        print("3. å¼€å§‹è®­ç»ƒ: python scripts/train.py")

        return 0
    else:
        print("âŒ ç¯å¢ƒé…ç½®ä¸å®Œæ•´ï¼Œè¯·å®‰è£…ç¼ºå¤±ç»„ä»¶")
        print("\\nğŸ’¡ å®‰è£…å‘½ä»¤:")
        print("pip install -r requirements.txt")

        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)