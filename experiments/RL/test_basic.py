#!/usr/bin/env python3
"""
åŸºç¡€æµ‹è¯•ï¼šéªŒè¯æ ¸å¿ƒç®—æ³•é€»è¾‘ï¼ˆæ— éœ€matplotlibï¼‰
"""

import numpy as np

# ç®€å•çš„å¤šè‡‚èµŒåšæœºæµ‹è¯•
class SimpleBandit:
    def __init__(self, n_arms=3):
        np.random.seed(42)
        self.true_values = np.random.normal(0, 1, n_arms)
        self.optimal_arm = np.argmax(self.true_values)
        print(f"ğŸ° åˆ›å»º{n_arms}è‡‚èµŒåšæœºï¼Œæœ€ä¼˜è‡‚: {self.optimal_arm}")
        print(f"çœŸå®æœŸæœ›å¥–åŠ±: {[f'{v:.2f}' for v in self.true_values]}")

    def pull(self, arm):
        return np.random.normal(self.true_values[arm], 1)

class SimpleEpsilonGreedy:
    def __init__(self, n_arms, epsilon=0.1):
        self.n_arms = n_arms
        self.epsilon = epsilon
        self.q_values = np.zeros(n_arms)
        self.action_counts = np.zeros(n_arms)

    def choose_action(self):
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_arms)
        else:
            return np.argmax(self.q_values)

    def update(self, action, reward):
        self.action_counts[action] += 1
        alpha = 1.0 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])

# è¿è¡Œç®€å•æµ‹è¯•
print("ğŸ§ª å¼€å§‹åŸºç¡€ç®—æ³•æµ‹è¯•...")

bandit = SimpleBandit(n_arms=3)
agent = SimpleEpsilonGreedy(n_arms=3, epsilon=0.1)

total_reward = 0
optimal_actions = 0
n_steps = 1000

for step in range(n_steps):
    action = agent.choose_action()
    reward = bandit.pull(action)
    agent.update(action, reward)

    total_reward += reward
    if action == bandit.optimal_arm:
        optimal_actions += 1

    # æ¯200æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
    if (step + 1) % 200 == 0:
        avg_reward = total_reward / (step + 1)
        optimal_rate = optimal_actions / (step + 1)
        print(f"æ­¥éª¤ {step+1:4d}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, æœ€ä¼˜ç‡={optimal_rate:.1%}")

print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
print(f"æœ€ç»ˆQå€¼ä¼°è®¡: {[f'{q:.2f}' for q in agent.q_values]}")
print(f"çœŸå®Qå€¼:     {[f'{v:.2f}' for v in bandit.true_values]}")
print(f"åŠ¨ä½œé€‰æ‹©æ¬¡æ•°: {agent.action_counts.astype(int)}")

# éªŒè¯å­¦ä¹ æ•ˆæœ
final_avg = total_reward / n_steps
final_optimal_rate = optimal_actions / n_steps
print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½:")
print(f"å¹³å‡å¥–åŠ±: {final_avg:.3f}")
print(f"æœ€ä¼˜åŠ¨ä½œç‡: {final_optimal_rate:.1%}")

if final_optimal_rate > 0.8:
    print("ğŸ† ä¼˜ç§€ï¼ç®—æ³•æˆåŠŸå­¦ä¹ åˆ°æœ€ä¼˜ç­–ç•¥")
elif final_optimal_rate > 0.6:
    print("ğŸ‘ ä¸é”™ï¼ç®—æ³•æœ‰æ˜æ˜¾å­¦ä¹ æ•ˆæœ")
else:
    print("ğŸ“š éœ€è¦è°ƒæ•´å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ­¥æ•°")

print("\nğŸ¯ æ ¸å¿ƒç®—æ³•é€»è¾‘éªŒè¯æˆåŠŸï¼")
print("ç°åœ¨å¯ä»¥å®‰å…¨åœ°è¿è¡Œå®Œæ•´ç‰ˆæœ¬çš„æ¡ˆä¾‹äº†ã€‚")