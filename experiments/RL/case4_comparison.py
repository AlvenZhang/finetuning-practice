#!/usr/bin/env python3
"""
æ¡ˆä¾‹4ï¼šç®—æ³•æ•ˆæœå¯¹æ¯” - ç†è§£ç®—æ³•é€‰æ‹©å’Œæ€§èƒ½å·®å¼‚

è¿™ä¸ªæ¡ˆä¾‹å¯¹æ¯”ä¸åŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼š
- åœ¨ç›¸åŒç¯å¢ƒä¸‹æµ‹è¯•å¤šç§ç®—æ³•
- ç›´è§‚å±•ç¤ºç®—æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯
- æä¾›ç®—æ³•é€‰æ‹©çš„æŒ‡å¯¼åŸåˆ™

è¿è¡Œæ—¶é—´ï¼šçº¦10åˆ†é’Ÿçœ‹åˆ°å…¨é¢å¯¹æ¯”
å­¦ä¹ ç›®æ ‡ï¼š
1. ç›´è§‚å¯¹æ¯”ä¸åŒç®—æ³•çš„æ€§èƒ½å·®å¼‚
2. ç†è§£ç®—æ³•é€‰æ‹©çš„è€ƒè™‘å› ç´ 
3. å»ºç«‹å¯¹RLç®—æ³•å…¨æ™¯çš„è®¤è¯†
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from typing import Dict, List, Tuple, Any
import time
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥ä¹‹å‰å®ç°çš„ç®—æ³•
import sys
import os

# ç¡®ä¿å¯ä»¥å¯¼å…¥ä¹‹å‰çš„æ¨¡å—
sys.path.append('/Users/xifeng/project/finetuning-0106/experiments/RL')

try:
    from case1_bandit import MultiArmBandit, EpsilonGreedyAgent, UCBAgent, GreedyAgent
    from case2_gridworld import GridWorld, QLearningAgent
    print("âœ… æˆåŠŸå¯¼å…¥ä¹‹å‰çš„ç®—æ³•æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ä¹‹å‰çš„æ¡ˆä¾‹æ–‡ä»¶å­˜åœ¨ä¸”å¯è¿è¡Œ")
    sys.exit(1)

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.style.use('default')
sns.set_palette("husl")

class AlgorithmComparator:
    """ç®—æ³•å¯¹æ¯”å™¨

    åœ¨ä¸åŒç¯å¢ƒä¸‹å¯¹æ¯”å„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½
    """

    def __init__(self):
        """åˆå§‹åŒ–å¯¹æ¯”å™¨"""
        self.results = {}
        print("ğŸ”¬ åˆå§‹åŒ–ç®—æ³•å¯¹æ¯”å™¨")

    def compare_bandit_algorithms(self, n_steps: int = 2000, n_runs: int = 10) -> Dict[str, Any]:
        """å¯¹æ¯”å¤šè‡‚èµŒåšæœºç®—æ³•

        Args:
            n_steps: æ¯æ¬¡è¿è¡Œçš„æ­¥æ•°
            n_runs: è¿è¡Œæ¬¡æ•°

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        print(f"\nğŸ° å¤šè‡‚èµŒåšæœºç®—æ³•å¯¹æ¯”")
        print(f"å‚æ•°ï¼š{n_steps}æ­¥ï¼Œ{n_runs}æ¬¡è¿è¡Œ")

        # åˆ›å»ºç¯å¢ƒ
        bandit = MultiArmBandit(n_arms=10, seed=42)

        # å®šä¹‰ç®—æ³•
        algorithms = {
            'Îµ-è´ªå¿ƒ (Îµ=0.1)': lambda: EpsilonGreedyAgent(10, epsilon=0.1),
            'Îµ-è´ªå¿ƒ (Îµ=0.05)': lambda: EpsilonGreedyAgent(10, epsilon=0.05),
            'UCB (c=2.0)': lambda: UCBAgent(10, c=2.0),
            'UCB (c=1.0)': lambda: UCBAgent(10, c=1.0),
            'çº¯è´ªå¿ƒ': lambda: GreedyAgent(10)
        }

        results = {}\n\n        for alg_name, alg_factory in algorithms.items():\n            print(f\"  æµ‹è¯•: {alg_name}\")\n\n            # å¤šæ¬¡è¿è¡Œæ±‚å¹³å‡\n            all_rewards = np.zeros((n_runs, n_steps))\n            all_optimal_actions = np.zeros((n_runs, n_steps))\n\n            for run in range(n_runs):\n                agent = alg_factory()\n                run_bandit = MultiArmBandit(n_arms=10, seed=42+run)\n\n                rewards = []\n                optimal_actions = []\n\n                for step in range(n_steps):\n                    action = agent.choose_action()\n                    reward = run_bandit.pull(action)\n                    agent.update(action, reward)\n\n                    rewards.append(reward)\n                    optimal_actions.append(1 if action == run_bandit.optimal_arm else 0)\n\n                all_rewards[run] = rewards\n                all_optimal_actions[run] = optimal_actions\n\n            # è®¡ç®—ç»Ÿè®¡æ•°æ®\n            avg_rewards = np.mean(all_rewards, axis=0)\n            avg_optimal_rate = np.mean(all_optimal_actions, axis=0)\n\n            results[alg_name] = {\n                'type': 'bandit',\n                'rewards': avg_rewards,\n                'optimal_rate': avg_optimal_rate,\n                'cumulative_reward': np.cumsum(avg_rewards),\n                'final_reward': np.mean(avg_rewards[-100:]),\n                'final_optimal_rate': np.mean(avg_optimal_rate[-100:]),\n                'convergence_step': self._find_convergence_step(avg_optimal_rate)\n            }\n\n        return results\n\n    def compare_gridworld_algorithms(self, episodes: int = 500, n_runs: int = 5) -> Dict[str, Any]:\n        \"\"\"å¯¹æ¯”ç½‘æ ¼ä¸–ç•Œç®—æ³•\n\n        Args:\n            episodes: è®­ç»ƒå›åˆæ•°\n            n_runs: è¿è¡Œæ¬¡æ•°\n\n        Returns:\n            å¯¹æ¯”ç»“æœ\n        \"\"\"\n        print(f\"\\nğŸ—ºï¸  ç½‘æ ¼ä¸–ç•Œç®—æ³•å¯¹æ¯”\")\n        print(f\"å‚æ•°ï¼š{episodes}å›åˆï¼Œ{n_runs}æ¬¡è¿è¡Œ\")\n\n        # å®šä¹‰ä¸åŒçš„Q-Learningé…ç½®\n        configs = [\n            {'lr': 0.1, 'epsilon': 0.1, 'name': 'Q-Learning (æ ‡å‡†)'},\n            {'lr': 0.05, 'epsilon': 0.1, 'name': 'Q-Learning (ä½å­¦ä¹ ç‡)'},\n            {'lr': 0.2, 'epsilon': 0.1, 'name': 'Q-Learning (é«˜å­¦ä¹ ç‡)'},\n            {'lr': 0.1, 'epsilon': 0.3, 'name': 'Q-Learning (é«˜æ¢ç´¢)'},\n            {'lr': 0.1, 'epsilon': 0.01, 'name': 'Q-Learning (ä½æ¢ç´¢)'}\n        ]\n\n        env = GridWorld(size=5)\n        n_states = env.size * env.size\n        results = {}\n\n        for config in configs:\n            print(f\"  æµ‹è¯•: {config['name']}\")\n\n            all_rewards = np.zeros((n_runs, episodes))\n            all_lengths = np.zeros((n_runs, episodes))\n            success_rates = []\n\n            for run in range(n_runs):\n                agent = QLearningAgent(n_states, env.n_actions,\n                                     learning_rate=config['lr'],\n                                     epsilon=config['epsilon'])\n\n                episode_rewards = []\n                episode_lengths = []\n                recent_successes = []\n\n                for episode in range(episodes):\n                    pos = env.reset()\n                    state = env.get_state_id(pos)\n                    total_reward = 0\n                    steps = 0\n\n                    while True:\n                        action = agent.choose_action(state)\n                        next_pos, reward, done = env.step(action)\n                        next_state = env.get_state_id(next_pos)\n\n                        agent.update(state, action, reward, next_state, done)\n\n                        total_reward += reward\n                        steps += 1\n                        state = next_state\n\n                        if done or steps > 100:\n                            break\n\n                    episode_rewards.append(total_reward)\n                    episode_lengths.append(steps)\n                    recent_successes.append(1 if done and steps <= 20 else 0)\n\n                    # è¡°å‡æ¢ç´¢ç‡\n                    agent.decay_epsilon(0.995)\n\n                all_rewards[run] = episode_rewards\n                all_lengths[run] = episode_lengths\n\n                # è®¡ç®—æœ€å100å›åˆçš„æˆåŠŸç‡\n                success_rate = np.mean(recent_successes[-100:]) if len(recent_successes) >= 100 else 0\n                success_rates.append(success_rate)\n\n            # è®¡ç®—ç»Ÿè®¡æ•°æ®\n            avg_rewards = np.mean(all_rewards, axis=0)\n            avg_lengths = np.mean(all_lengths, axis=0)\n\n            results[config['name']] = {\n                'type': 'gridworld',\n                'rewards': avg_rewards,\n                'lengths': avg_lengths,\n                'final_reward': np.mean(avg_rewards[-50:]),\n                'final_length': np.mean(avg_lengths[-50:]),\n                'success_rate': np.mean(success_rates),\n                'convergence_step': self._find_convergence_step(avg_rewards, threshold_type='reward')\n            }\n\n        return results\n\n    def _find_convergence_step(self, values: np.ndarray, threshold_type: str = 'optimal_rate') -> int:\n        \"\"\"å¯»æ‰¾ç®—æ³•æ”¶æ•›çš„æ­¥æ•°\n\n        Args:\n            values: æ€§èƒ½æŒ‡æ ‡åºåˆ—\n            threshold_type: é˜ˆå€¼ç±»å‹\n\n        Returns:\n            æ”¶æ•›æ­¥æ•°\n        \"\"\"\n        if threshold_type == 'optimal_rate':\n            # å¯¹äºæœ€ä¼˜åŠ¨ä½œç‡ï¼Œå¯»æ‰¾è¾¾åˆ°80%çš„æ­¥æ•°\n            threshold = 0.8\n            target_length = 50  # è¿ç»­50æ­¥ä¿æŒåœ¨é˜ˆå€¼ä»¥ä¸Š\n        else:\n            # å¯¹äºå¥–åŠ±ï¼Œå¯»æ‰¾è¾¾åˆ°ç¨³å®šçš„æ­¥æ•°\n            threshold = np.mean(values[-100:]) * 0.9  # 90%çš„æœ€ç»ˆæ€§èƒ½\n            target_length = 50\n\n        for i in range(len(values) - target_length):\n            if np.all(values[i:i+target_length] >= threshold):\n                return i\n\n        return len(values)  # æœªæ”¶æ•›\n\n    def create_comprehensive_comparison(self) -> None:\n        \"\"\"åˆ›å»ºç»¼åˆå¯¹æ¯”åˆ†æ\"\"\"\n        print(f\"\\nğŸ“Š å¼€å§‹ç»¼åˆç®—æ³•å¯¹æ¯”åˆ†æ\")\n        start_time = time.time()\n\n        # è¿è¡Œæ‰€æœ‰å¯¹æ¯”å®éªŒ\n        bandit_results = self.compare_bandit_algorithms(n_steps=1500, n_runs=8)\n        gridworld_results = self.compare_gridworld_algorithms(episodes=400, n_runs=3)\n\n        # åˆå¹¶ç»“æœ\n        all_results = {**bandit_results, **gridworld_results}\n        self.results = all_results\n\n        elapsed_time = time.time() - start_time\n        print(f\"\\nâ±ï¸  å¯¹æ¯”å®éªŒå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f}ç§’\")\n\n        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š\n        self.visualize_comprehensive_results()\n        self.generate_algorithm_guide()\n\n    def visualize_comprehensive_results(self) -> None:\n        \"\"\"å¯è§†åŒ–ç»¼åˆå¯¹æ¯”ç»“æœ\"\"\"\n        print(f\"\\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š\")\n\n        # åˆ›å»ºå¤§å‹å›¾è¡¨\n        fig = plt.figure(figsize=(20, 15))\n        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n\n        # 1. å¤šè‡‚èµŒåšæœºï¼šç´¯ç§¯å¥–åŠ±å¯¹æ¯”\n        ax1 = fig.add_subplot(gs[0, :2])\n        bandit_algos = {k: v for k, v in self.results.items() if v['type'] == 'bandit'}\n        for name, data in bandit_algos.items():\n            ax1.plot(data['cumulative_reward'], label=name, linewidth=2)\n        ax1.set_xlabel('æ—¶é—´æ­¥')\n        ax1.set_ylabel('ç´¯ç§¯å¥–åŠ±')\n        ax1.set_title('å¤šè‡‚èµŒåšæœºï¼šç´¯ç§¯å¥–åŠ±å¯¹æ¯”', fontsize=14, fontweight='bold')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n\n        # 2. å¤šè‡‚èµŒåšæœºï¼šæœ€ä¼˜åŠ¨ä½œç‡å¯¹æ¯”\n        ax2 = fig.add_subplot(gs[0, 2:])\n        for name, data in bandit_algos.items():\n            # è®¡ç®—æ»‘åŠ¨å¹³å‡\n            window_size = 100\n            moving_avg = np.convolve(data['optimal_rate'], np.ones(window_size)/window_size, mode='valid')\n            x = np.arange(window_size-1, len(data['optimal_rate']))\n            ax2.plot(x, moving_avg, label=name, linewidth=2)\n        ax2.set_xlabel('æ—¶é—´æ­¥')\n        ax2.set_ylabel('æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡')\n        ax2.set_title('å¤šè‡‚èµŒåšæœºï¼šæœ€ä¼˜åŠ¨ä½œç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        ax2.set_ylim(0, 1)\n\n        # 3. ç½‘æ ¼ä¸–ç•Œï¼šå­¦ä¹ æ›²çº¿å¯¹æ¯”\n        ax3 = fig.add_subplot(gs[1, :2])\n        gridworld_algos = {k: v for k, v in self.results.items() if v['type'] == 'gridworld'}\n        for name, data in gridworld_algos.items():\n            # è®¡ç®—æ»‘åŠ¨å¹³å‡\n            window_size = 50\n            rewards = data['rewards']\n            if len(rewards) > window_size:\n                moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n                x = np.arange(window_size-1, len(rewards))\n                ax3.plot(x, moving_avg, label=name.replace('Q-Learning ', ''), linewidth=2)\n        ax3.set_xlabel('å›åˆ')\n        ax3.set_ylabel('å¹³å‡å¥–åŠ±')\n        ax3.set_title('ç½‘æ ¼ä¸–ç•Œï¼šå­¦ä¹ æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n\n        # 4. ç½‘æ ¼ä¸–ç•Œï¼šå›åˆé•¿åº¦å¯¹æ¯”\n        ax4 = fig.add_subplot(gs[1, 2:])\n        for name, data in gridworld_algos.items():\n            # è®¡ç®—æ»‘åŠ¨å¹³å‡\n            window_size = 50\n            lengths = data['lengths']\n            if len(lengths) > window_size:\n                moving_avg = np.convolve(lengths, np.ones(window_size)/window_size, mode='valid')\n                x = np.arange(window_size-1, len(lengths))\n                ax4.plot(x, moving_avg, label=name.replace('Q-Learning ', ''), linewidth=2)\n        ax4.set_xlabel('å›åˆ')\n        ax4.set_ylabel('å¹³å‡æ­¥æ•°')\n        ax4.set_title('ç½‘æ ¼ä¸–ç•Œï¼šå›åˆé•¿åº¦å¯¹æ¯”', fontsize=14, fontweight='bold')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n\n        # 5. æ€§èƒ½æ€»ç»“çƒ­åŠ›å›¾\n        ax5 = fig.add_subplot(gs[2, :])\n        self._create_performance_heatmap(ax5)\n\n        plt.suptitle('å¼ºåŒ–å­¦ä¹ ç®—æ³•ç»¼åˆå¯¹æ¯”åˆ†æ', fontsize=18, fontweight='bold', y=0.98)\n\n        # ä¿å­˜å›¾ç‰‡\n        plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case4_comprehensive_comparison.png',\n                   dpi=300, bbox_inches='tight')\n        print(f\"ğŸ“Š ç»¼åˆå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: results/plots/case4_comprehensive_comparison.png\")\n        plt.show()\n\n    def _create_performance_heatmap(self, ax) -> None:\n        \"\"\"åˆ›å»ºæ€§èƒ½æ€»ç»“çƒ­åŠ›å›¾\"\"\"\n        # å‡†å¤‡æ•°æ®\n        metrics = []\n        algorithm_names = []\n\n        for name, data in self.results.items():\n            algorithm_names.append(name)\n            if data['type'] == 'bandit':\n                # å¤šè‡‚èµŒåšæœºæŒ‡æ ‡ï¼ˆæ ‡å‡†åŒ–åˆ°0-1ï¼‰\n                final_reward_norm = min(data['final_reward'] / 1.5, 1.0)  # å‡è®¾æœ€å¤§æœŸæœ›å¥–åŠ±ä¸º1.5\n                optimal_rate = data['final_optimal_rate']\n                convergence_speed = max(0, 1 - data['convergence_step'] / 1000)  # è¶Šå¿«æ”¶æ•›åˆ†æ•°è¶Šé«˜\n                metrics.append([final_reward_norm, optimal_rate, convergence_speed])\n            else:\n                # ç½‘æ ¼ä¸–ç•ŒæŒ‡æ ‡ï¼ˆæ ‡å‡†åŒ–åˆ°0-1ï¼‰\n                reward_norm = min((data['final_reward'] + 10) / 10, 1.0)  # å¥–åŠ±é€šå¸¸æ˜¯è´Ÿæ•°\n                efficiency = max(0, min(1.0, (50 - data['final_length']) / 40))  # æ­¥æ•°è¶Šå°‘è¶Šå¥½\n                success_rate = data['success_rate']\n                metrics.append([reward_norm, efficiency, success_rate])\n\n        # åˆ›å»ºDataFrame\n        if any(data['type'] == 'bandit' for data in self.results.values()):\n            columns = ['æœ€ç»ˆæ€§èƒ½', 'æœ€ä¼˜ç‡/æ•ˆç‡', 'æ”¶æ•›é€Ÿåº¦/æˆåŠŸç‡']\n        else:\n            columns = ['æœ€ç»ˆå¥–åŠ±', 'æ•ˆç‡', 'æˆåŠŸç‡']\n\n        df = pd.DataFrame(metrics, index=algorithm_names, columns=columns)\n\n        # ç»˜åˆ¶çƒ­åŠ›å›¾\n        im = ax.imshow(df.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n\n        # è®¾ç½®æ ‡ç­¾\n        ax.set_xticks(range(len(columns)))\n        ax.set_yticks(range(len(algorithm_names)))\n        ax.set_xticklabels(columns)\n        ax.set_yticklabels(algorithm_names)\n\n        # æ·»åŠ æ•°å€¼æ ‡æ³¨\n        for i in range(len(algorithm_names)):\n            for j in range(len(columns)):\n                value = df.values[i, j]\n                ax.text(j, i, f'{value:.2f}', ha='center', va='center',\n                       color='white' if value < 0.5 else 'black', fontweight='bold')\n\n        ax.set_title('ç®—æ³•æ€§èƒ½ç»¼åˆè¯„åˆ† (0-1æ ‡å‡†åŒ–)', fontsize=14, fontweight='bold')\n\n        # æ·»åŠ é¢œè‰²æ¡\n        cbar = plt.colorbar(im, ax=ax, shrink=0.6)\n        cbar.set_label('æ€§èƒ½åˆ†æ•°', rotation=270, labelpad=15)\n\n    def generate_algorithm_guide(self) -> None:\n        \"\"\"ç”Ÿæˆç®—æ³•é€‰æ‹©æŒ‡å—\"\"\"\n        print(f\"\\nğŸ“‹ ç”Ÿæˆç®—æ³•é€‰æ‹©æŒ‡å—\")\n\n        guide_content = \"\"\"\n# å¼ºåŒ–å­¦ä¹ ç®—æ³•é€‰æ‹©æŒ‡å—\n\n## ğŸ“Š å®éªŒç»“æœæ€»ç»“\n\n### å¤šè‡‚èµŒåšæœºç®—æ³•å¯¹æ¯”ï¼š\n\"\"\"\n\n        # åˆ†æå¤šè‡‚èµŒåšæœºç»“æœ\n        bandit_algos = {k: v for k, v in self.results.items() if v['type'] == 'bandit'}\n        bandit_ranking = sorted(bandit_algos.items(), key=lambda x: x[1]['final_reward'], reverse=True)\n\n        guide_content += \"\\n**æœ€ç»ˆæ€§èƒ½æ’åï¼ˆæŒ‰å¹³å‡å¥–åŠ±ï¼‰ï¼š**\\n\"\n        for i, (name, data) in enumerate(bandit_ranking, 1):\n            guide_content += f\"{i}. {name}: {data['final_reward']:.3f} (æœ€ä¼˜ç‡: {data['final_optimal_rate']:.1%})\\n\"\n\n        # åˆ†æç½‘æ ¼ä¸–ç•Œç»“æœ\n        gridworld_algos = {k: v for k, v in self.results.items() if v['type'] == 'gridworld'}\n        if gridworld_algos:\n            gridworld_ranking = sorted(gridworld_algos.items(), key=lambda x: x[1]['final_reward'], reverse=True)\n\n            guide_content += \"\\n### ç½‘æ ¼ä¸–ç•Œç®—æ³•å¯¹æ¯”ï¼š\\n\"\n            guide_content += \"\\n**æœ€ç»ˆæ€§èƒ½æ’åï¼ˆæŒ‰å¹³å‡å¥–åŠ±ï¼‰ï¼š**\\n\"\n            for i, (name, data) in enumerate(gridworld_ranking, 1):\n                guide_content += f\"{i}. {name}: {data['final_reward']:.2f} (å¹³å‡æ­¥æ•°: {data['final_length']:.1f})\\n\"\n\n        # æ·»åŠ é€‰æ‹©å»ºè®®\n        guide_content += \"\"\"\n\n## ğŸ¯ ç®—æ³•é€‰æ‹©å»ºè®®\n\n### å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼š\n- **UCBç®—æ³•**ï¼šæ™ºèƒ½æ¢ç´¢ï¼Œè‡ªé€‚åº”å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œé€šå¸¸æ€§èƒ½æœ€ä½³\n- **Îµ-è´ªå¿ƒ (Îµ=0.1)**ï¼šç®€å•æœ‰æ•ˆï¼Œé€‚åˆå¿«é€ŸåŸå‹å¼€å‘\n- **Îµ-è´ªå¿ƒ (Îµ=0.05)**ï¼šæ›´ä¿å®ˆçš„æ¢ç´¢ï¼Œé€‚åˆå¥–åŠ±å·®å¼‚æ˜æ˜¾çš„ç¯å¢ƒ\n- **çº¯è´ªå¿ƒ**ï¼šæ”¶æ•›å¿«ä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä¸æ¨è\n\n### Q-Learningå‚æ•°è°ƒä¼˜ï¼š\n- **å­¦ä¹ ç‡Î±**ï¼š0.1æ˜¯å¥½çš„èµ·ç‚¹ï¼Œå¤æ‚ç¯å¢ƒå¯é™ä½åˆ°0.05\n- **æ¢ç´¢ç‡Îµ**ï¼šä»0.3å¼€å§‹ï¼Œé€æ¸è¡°å‡åˆ°0.01\n- **æŠ˜æ‰£å› å­Î³**ï¼šé€šå¸¸è®¾ä¸º0.95-0.99\n\n### ä¸€èˆ¬æ€§æŒ‡å¯¼åŸåˆ™ï¼š\n\n#### 1. ç¯å¢ƒç‰¹ç‚¹ vs ç®—æ³•é€‰æ‹©\n- **ç¦»æ•£å°çŠ¶æ€ç©ºé—´** â†’ è¡¨æ ¼å¼Q-Learning\n- **è¿ç»­/é«˜ç»´çŠ¶æ€** â†’ DQNæˆ–ç­–ç•¥æ¢¯åº¦æ–¹æ³•\n- **æ¢ç´¢æˆæœ¬é«˜** â†’ UCBæˆ–Thompsoné‡‡æ ·\n- **éœ€è¦å¿«é€Ÿæ”¶æ•›** â†’ è¾ƒé«˜å­¦ä¹ ç‡ + é€‚åº¦æ¢ç´¢\n\n#### 2. æ€§èƒ½è¦æ±‚ vs å¤æ‚åº¦æƒè¡¡\n- **å¿«é€ŸåŸå‹** â†’ Îµ-è´ªå¿ƒ + Q-Learning\n- **æœ€ä¼˜æ€§èƒ½** â†’ UCB + å‚æ•°è°ƒä¼˜\n- **ç¨³å®šæ€§ä¼˜å…ˆ** â†’ è¾ƒä½å­¦ä¹ ç‡ + é•¿æœŸè®­ç»ƒ\n- **å®æ—¶åº”ç”¨** â†’ é¢„è®­ç»ƒæ¨¡å‹ + åœ¨çº¿å¾®è°ƒ\n\n#### 3. è°ƒè¯•å’Œä¼˜åŒ–å»ºè®®\n- **å­¦ä¹ ä¸ç¨³å®š** â†’ é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ¢ç´¢\n- **æ”¶æ•›å¤ªæ…¢** â†’ æé«˜å­¦ä¹ ç‡ï¼Œæ£€æŸ¥å¥–åŠ±è®¾è®¡\n- **æ€§èƒ½å¹³å°æœŸ** â†’ è°ƒæ•´æ¢ç´¢ç­–ç•¥ï¼Œæ£€æŸ¥ç‰¹å¾è¡¨ç¤º\n- **è¿‡æ‹Ÿåˆ** â†’ å¢åŠ æ¢ç´¢ï¼Œä½¿ç”¨ç»éªŒå›æ”¾\n\n## ğŸ” æ·±å…¥å­¦ä¹ æ–¹å‘\n\n### ç†è®ºæ·±å…¥ï¼š\n- é—æ†¾ç•Œç†è®ºï¼ˆRegret Boundsï¼‰\n- æ”¶æ•›æ€§åˆ†æ\n- æ ·æœ¬å¤æ‚åº¦ç†è®º\n\n### ç®—æ³•æ‰©å±•ï¼š\n- å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ \n- åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ \n- å…ƒå­¦ä¹ å’Œè¿ç§»å­¦ä¹ \n\n### åº”ç”¨é¢†åŸŸï¼š\n- æ¨èç³»ç»Ÿï¼ˆå¤šè‡‚èµŒåšæœºï¼‰\n- æ¸¸æˆAIï¼ˆQ-Learning/DQNï¼‰\n- æœºå™¨äººæ§åˆ¶ï¼ˆè¿ç»­æ§åˆ¶ï¼‰\n- é‡‘èäº¤æ˜“ï¼ˆé£é™©ç®¡ç†ï¼‰\n\"\"\"\n\n        # ä¿å­˜æŒ‡å—\n        guide_path = '/Users/xifeng/project/finetuning-0106/experiments/RL/results/algorithm_selection_guide.md'\n        with open(guide_path, 'w', encoding='utf-8') as f:\n            f.write(guide_content)\n\n        print(f\"ğŸ“– ç®—æ³•é€‰æ‹©æŒ‡å—å·²ä¿å­˜åˆ°: {guide_path}\")\n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸ“š æ ¸å¿ƒæ´å¯Ÿæ€»ç»“\")\n        print(\"=\"*60)\n\n        # æ‰“å°æ ¸å¿ƒæ´å¯Ÿ\n        if bandit_algos:\n            best_bandit = bandit_ranking[0]\n            print(f\"ğŸ† å¤šè‡‚èµŒåšæœºæœ€ä½³ç®—æ³•: {best_bandit[0]}\")\n            print(f\"   æ€§èƒ½: {best_bandit[1]['final_reward']:.3f} (æœ€ä¼˜ç‡: {best_bandit[1]['final_optimal_rate']:.1%})\")\n\n        if gridworld_algos:\n            best_gridworld = gridworld_ranking[0]\n            print(f\"ğŸ† ç½‘æ ¼ä¸–ç•Œæœ€ä½³é…ç½®: {best_gridworld[0]}\")\n            print(f\"   æ€§èƒ½: {best_gridworld[1]['final_reward']:.2f} (å¹³å‡æ­¥æ•°: {best_gridworld[1]['final_length']:.1f})\")\n\n        print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n        print(\"  â€¢ æ¢ç´¢ç­–ç•¥çš„é€‰æ‹©å¯¹é•¿æœŸæ€§èƒ½è‡³å…³é‡è¦\")\n        print(\"  â€¢ å‚æ•°è°ƒä¼˜éœ€è¦å¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½\")\n        print(\"  â€¢ ä¸åŒç¯å¢ƒéœ€è¦ä¸åŒçš„ç®—æ³•ç­–ç•¥\")\n        print(\"  â€¢ å¯è§†åŒ–åˆ†ææ˜¯ç†è§£ç®—æ³•è¡Œä¸ºçš„é‡è¦å·¥å…·\")\n\ndef create_quick_demo() -> None:\n    \"\"\"åˆ›å»ºå¿«é€Ÿæ¼”ç¤ºï¼Œå±•ç¤ºå…³é”®ç®—æ³•å·®å¼‚\"\"\"\n    print(f\"\\nğŸ¬ å¿«é€Ÿæ¼”ç¤ºï¼šå…³é”®ç®—æ³•å·®å¼‚\")\n\n    # ç®€åŒ–çš„å¯¹æ¯”å®éªŒ\n    bandit = MultiArmBandit(n_arms=5, seed=42)\n    agents = {\n        'è´ªå¿ƒ': GreedyAgent(5),\n        'Îµ-è´ªå¿ƒ': EpsilonGreedyAgent(5, epsilon=0.1),\n        'UCB': UCBAgent(5, c=2.0)\n    }\n\n    n_steps = 500\n    results = {name: {'rewards': [], 'actions': []} for name in agents.keys()}\n\n    # è¿è¡Œæ¼”ç¤º\n    for step in range(n_steps):\n        for name, agent in agents.items():\n            action = agent.choose_action()\n            reward = bandit.pull(action)\n            agent.update(action, reward)\n\n            results[name]['rewards'].append(reward)\n            results[name]['actions'].append(action)\n\n    # å¿«é€Ÿå¯è§†åŒ–\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # ç´¯ç§¯å¥–åŠ±\n    for name, data in results.items():\n        cumulative = np.cumsum(data['rewards'])\n        ax1.plot(cumulative, label=name, linewidth=2)\n\n    ax1.set_xlabel('æ—¶é—´æ­¥')\n    ax1.set_ylabel('ç´¯ç§¯å¥–åŠ±')\n    ax1.set_title('ç®—æ³•æ€§èƒ½å¿«é€Ÿå¯¹æ¯”')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # åŠ¨ä½œé€‰æ‹©åˆ†å¸ƒ\n    for i, (name, data) in enumerate(results.items()):\n        action_counts = np.bincount(data['actions'], minlength=5)\n        ax2.bar(np.arange(5) + i*0.25, action_counts, width=0.25, label=name, alpha=0.8)\n\n    ax2.set_xlabel('è‡‚ç¼–å·')\n    ax2.set_ylabel('é€‰æ‹©æ¬¡æ•°')\n    ax2.set_title('åŠ¨ä½œé€‰æ‹©åˆ†å¸ƒ')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # æ ‡æ³¨æœ€ä¼˜è‡‚\n    ax2.axvline(x=bandit.optimal_arm, color='red', linestyle='--', alpha=0.7, label='æœ€ä¼˜è‡‚')\n\n    plt.tight_layout()\n    plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case4_quick_demo.png',\n               dpi=300, bbox_inches='tight')\n    plt.show()\n\n    # æ‰“å°ç»“æœ\n    print(f\"\\nğŸ“Š æ¼”ç¤ºç»“æœ ({n_steps}æ­¥):\")\n    for name, data in results.items():\n        total_reward = sum(data['rewards'])\n        optimal_actions = sum(1 for a in data['actions'] if a == bandit.optimal_arm)\n        optimal_rate = optimal_actions / len(data['actions'])\n        print(f\"{name:8s}: æ€»å¥–åŠ±={total_reward:6.1f}, æœ€ä¼˜ç‡={optimal_rate:.1%}\")\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ğŸ“Š å¼ºåŒ–å­¦ä¹ ç®—æ³•ç»¼åˆå¯¹æ¯”åˆ†æ\")\n    print(\"=\" * 45)\n\n    # è¯¢é—®ç”¨æˆ·é€‰æ‹©\n    print(\"\\né€‰æ‹©è¿è¡Œæ¨¡å¼:\")\n    print(\"1. å¿«é€Ÿæ¼”ç¤º (2åˆ†é’Ÿ)\")\n    print(\"2. å®Œæ•´å¯¹æ¯”åˆ†æ (10åˆ†é’Ÿ)\")\n\n    try:\n        choice = input(\"\\nè¯·é€‰æ‹© (1/2): \").strip()\n\n        if choice == '1':\n            create_quick_demo()\n        elif choice == '2':\n            # åˆ›å»ºå¯¹æ¯”å™¨å¹¶è¿è¡Œå®Œæ•´åˆ†æ\n            comparator = AlgorithmComparator()\n            comparator.create_comprehensive_comparison()\n        else:\n            print(\"æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå¿«é€Ÿæ¼”ç¤º\")\n            create_quick_demo()\n\n    except KeyboardInterrupt:\n        print(\"\\nç”¨æˆ·å–æ¶ˆ\")\n        return\n\n    print(\"\\nâœ… æ¡ˆä¾‹4å®Œæˆï¼\")\n    print(\"ğŸ“ ä½ å­¦åˆ°äº†ï¼š\")\n    print(\"  â€¢ ä¸åŒç®—æ³•åœ¨ç›¸åŒç¯å¢ƒä¸‹çš„æ€§èƒ½å·®å¼‚\")\n    print(\"  â€¢ ç®—æ³•é€‰æ‹©éœ€è¦è€ƒè™‘ç¯å¢ƒç‰¹ç‚¹å’Œæ€§èƒ½è¦æ±‚\")\n    print(\"  â€¢ å‚æ•°è°ƒä¼˜å¯¹ç®—æ³•æ€§èƒ½çš„é‡è¦å½±å“\")\n    print(\"  â€¢ å¯è§†åŒ–åˆ†æå¸®åŠ©ç†è§£ç®—æ³•è¡Œä¸º\")\n\n    print(\"\\nğŸ‰ æ­å–œï¼ä½ å·²ç»å®Œæˆäº†å¼ºåŒ–å­¦ä¹ å¿«é€Ÿå®è·µä¹‹æ—…ï¼\")\n    print(\"\\nğŸ“š åç»­å­¦ä¹ å»ºè®®ï¼š\")\n    print(\"  â€¢ å°è¯•è°ƒæ•´å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ€§èƒ½çš„å½±å“\")\n    print(\"  â€¢ åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­æµ‹è¯•ç®—æ³•\")\n    print(\"  â€¢ å­¦ä¹ æ›´é«˜çº§çš„ç®—æ³•ï¼ˆPPOã€SACç­‰ï¼‰\")\n    print(\"  â€¢ åº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­\")\n\nif __name__ == \"__main__\":\n    main()