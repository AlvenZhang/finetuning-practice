#!/usr/bin/env python3
"""
æ¡ˆä¾‹1ï¼šå¤šè‡‚èµŒåšæœº - ç†è§£æ¢ç´¢vsåˆ©ç”¨æƒè¡¡

è¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æœ€åŸºç¡€çš„é—®é¢˜ï¼š
- æœ‰10ä¸ªæ‹‰æ†ï¼ˆè‡‚ï¼‰ï¼Œæ¯ä¸ªæœ‰ä¸åŒçš„å¥–åŠ±åˆ†å¸ƒ
- Agentéœ€è¦å­¦ä¹ å“ªä¸ªè‡‚çš„æœŸæœ›å¥–åŠ±æœ€é«˜
- æ ¸å¿ƒæŒ‘æˆ˜ï¼šæ¢ç´¢æ–°é€‰æ‹© vs åˆ©ç”¨å·²çŸ¥æœ€å¥½çš„é€‰æ‹©

è¿è¡Œæ—¶é—´ï¼šçº¦5åˆ†é’Ÿçœ‹åˆ°æ˜æ˜¾æ•ˆæœ
å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£æ¢ç´¢vsåˆ©ç”¨çš„æ ¹æœ¬æƒè¡¡
2. å¯¹æ¯”ä¸åŒç­–ç•¥çš„æ€§èƒ½å·®å¼‚
3. ç†è§£å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬è®¾å®š
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple
import time

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MultiArmBandit:
    """å¤šè‡‚èµŒåšæœºç¯å¢ƒ

    æ¯ä¸ªè‡‚æœ‰ä¸åŒçš„å¥–åŠ±åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼ŒAgentéœ€è¦æ‰¾åˆ°æœ€ä¼˜è‡‚
    """

    def __init__(self, n_arms: int = 10, seed: int = 42):
        """åˆå§‹åŒ–èµŒåšæœº

        Args:
            n_arms: è‡‚çš„æ•°é‡
            seed: éšæœºç§å­
        """
        np.random.seed(seed)
        self.n_arms = n_arms

        # æ¯ä¸ªè‡‚çš„çœŸå®æœŸæœ›å¥–åŠ±ï¼ˆAgentä¸çŸ¥é“ï¼‰
        self.true_values = np.random.normal(0, 1, n_arms)
        self.optimal_arm = np.argmax(self.true_values)

        print(f"ğŸ° åˆ›å»ºäº†{n_arms}è‡‚èµŒåšæœº")
        print(f"çœŸå®æœ€ä¼˜è‡‚: {self.optimal_arm} (æœŸæœ›å¥–åŠ±: {self.true_values[self.optimal_arm]:.3f})")
        print(f"æ‰€æœ‰è‡‚æœŸæœ›å¥–åŠ±: {[f'{v:.2f}' for v in self.true_values]}")

    def pull(self, arm: int) -> float:
        """æ‹‰åŠ¨æŒ‡å®šçš„è‡‚ï¼Œè¿”å›å¥–åŠ±

        Args:
            arm: è¦æ‹‰åŠ¨çš„è‡‚ç¼–å·

        Returns:
            ä»è¯¥è‡‚è·å¾—çš„å¥–åŠ±ï¼ˆåŠ äº†å™ªå£°ï¼‰
        """
        if arm < 0 or arm >= self.n_arms:
            raise ValueError(f"è‡‚ç¼–å·å¿…é¡»åœ¨0-{self.n_arms-1}ä¹‹é—´")

        # è¿”å›çœŸå®æœŸæœ›å€¼ + å™ªå£°
        reward = np.random.normal(self.true_values[arm], 1)
        return reward

class EpsilonGreedyAgent:
    """Îµ-è´ªå¿ƒç­–ç•¥Agent

    ä»¥Îµçš„æ¦‚ç‡éšæœºæ¢ç´¢ï¼Œä»¥(1-Îµ)çš„æ¦‚ç‡é€‰æ‹©å½“å‰ä¼°è®¡æœ€å¥½çš„è‡‚
    """

    def __init__(self, n_arms: int, epsilon: float = 0.1):
        """åˆå§‹åŒ–Agent

        Args:
            n_arms: è‡‚çš„æ•°é‡
            epsilon: æ¢ç´¢æ¦‚ç‡
        """
        self.n_arms = n_arms
        self.epsilon = epsilon
        self.q_values = np.zeros(n_arms)  # æ¯ä¸ªè‡‚çš„ä»·å€¼ä¼°è®¡
        self.action_counts = np.zeros(n_arms)  # æ¯ä¸ªè‡‚è¢«é€‰æ‹©çš„æ¬¡æ•°

    def choose_action(self) -> int:
        """æ ¹æ®Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            return np.random.randint(self.n_arms)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©å½“å‰ä¼°è®¡æœ€å¥½çš„è‡‚
            return np.argmax(self.q_values)

    def update(self, action: int, reward: float):
        """æ›´æ–°ä»·å€¼ä¼°è®¡"""
        self.action_counts[action] += 1
        # å¢é‡æ›´æ–°ï¼šQ(a) = Q(a) + Î±[R - Q(a)]ï¼Œå…¶ä¸­Î± = 1/N(a)
        alpha = 1.0 / self.action_counts[action]
        # reward - self.q_values[action]æ˜¯é¢„æµ‹è¯¯å·®ï¼Œæ­£è¯¯å·®è¡¨ç¤ºå®é™…å¥–åŠ±å¥½äºé¢„æœŸï¼Œè¡¨æ˜ä½ä¼°äº†è¯¥åŠ¨ä½œä»·å€¼ï¼Œéœ€è¦æé«˜self.q_values[action]ã€‚åä¹‹åˆ™ç›¸å
        self.q_values[action] += alpha * (reward - self.q_values[action])

class UCBAgent:
    """Upper Confidence Bound (UCB) ç­–ç•¥Agent

    é€‰æ‹©å…·æœ‰æœ€é«˜ä¸Šç½®ä¿¡ç•Œçš„è‡‚ï¼šQ(a) + c*sqrt(ln(t)/N(a))
    è‡ªåŠ¨å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨
    """

    def __init__(self, n_arms: int, c: float = 2.0):
        """åˆå§‹åŒ–Agent

        Args:
            n_arms: è‡‚çš„æ•°é‡
            c: ç½®ä¿¡åº¦å‚æ•°ï¼Œæ§åˆ¶æ¢ç´¢ç¨‹åº¦
        """
        self.n_arms = n_arms
        self.c = c
        self.q_values = np.zeros(n_arms)
        self.action_counts = np.zeros(n_arms)
        self.t = 0  # æ€»æ—¶é—´æ­¥

    def choose_action(self) -> int:
        """æ ¹æ®UCBç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        self.t += 1

        # å¦‚æœæœ‰è‡‚è¿˜æ²¡è¢«é€‰è¿‡ï¼Œå…ˆé€‰æ‹©å®ƒä»¬
        for a in range(self.n_arms):
            if self.action_counts[a] == 0:
                return a

        # è®¡ç®—UCBå€¼ï¼šQ(a) + c*sqrt(ln(t)/N(a))
        ucb_values = self.q_values + self.c * np.sqrt(
            np.log(self.t) / self.action_counts
        )
        return np.argmax(ucb_values)

    def update(self, action: int, reward: float):
        """æ›´æ–°ä»·å€¼ä¼°è®¡"""
        self.action_counts[action] += 1
        alpha = 1.0 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])

class GreedyAgent:
    """çº¯è´ªå¿ƒç­–ç•¥Agentï¼ˆä¸æ¢ç´¢ï¼‰

    æ€»æ˜¯é€‰æ‹©å½“å‰ä¼°è®¡æœ€å¥½çš„è‡‚ï¼Œä½œä¸ºå¯¹æ¯”åŸºçº¿
    """

    def __init__(self, n_arms: int):
        self.n_arms = n_arms
        self.q_values = np.zeros(n_arms)
        self.action_counts = np.zeros(n_arms)

    def choose_action(self) -> int:
        """é€‰æ‹©å½“å‰ä¼°è®¡æœ€å¥½çš„è‡‚"""
        # å¦‚æœæœ‰è‡‚è¿˜æ²¡è¢«é€‰è¿‡ï¼Œéšæœºé€‰ä¸€ä¸ª
        if np.min(self.action_counts) == 0:
            unselected = np.where(self.action_counts == 0)[0]
            return np.random.choice(unselected)

        return np.argmax(self.q_values)

    def update(self, action: int, reward: float):
        """æ›´æ–°ä»·å€¼ä¼°è®¡"""
        self.action_counts[action] += 1
        alpha = 1.0 / self.action_counts[action]
        self.q_values[action] += alpha * (reward - self.q_values[action])

def run_experiment(n_steps: int = 2000, n_runs: int = 10) -> None:
    """è¿è¡Œå¤šè‡‚èµŒåšæœºå®éªŒ

    Args:
        n_steps: æ¯æ¬¡è¿è¡Œçš„æ­¥æ•°
        n_runs: è¿è¡Œæ¬¡æ•°ï¼ˆç”¨äºå¹³å‡ï¼‰
    """
    print(f"\nğŸš€ å¼€å§‹å¤šè‡‚èµŒåšæœºå®éªŒ")
    print(f"å‚æ•°ï¼š{n_steps}æ­¥ï¼Œ{n_runs}æ¬¡è¿è¡Œå¹³å‡")

    # åˆ›å»ºç¯å¢ƒ
    bandit = MultiArmBandit(n_arms=10, seed=42)

    # åˆ›å»ºä¸åŒç­–ç•¥çš„Agent
    agents = {
        'Îµ-è´ªå¿ƒ (Îµ=0.1)': lambda: EpsilonGreedyAgent(10, epsilon=0.1),
        'Îµ-è´ªå¿ƒ (Îµ=0.01)': lambda: EpsilonGreedyAgent(10, epsilon=0.01),
        'UCB (c=2.0)': lambda: UCBAgent(10, c=2.0),
        'çº¯è´ªå¿ƒ': lambda: GreedyAgent(10)
    }

    # å­˜å‚¨ç»“æœ
    results = {}

    # ä¸ºæ¯ä¸ªç­–ç•¥è¿è¡Œå®éªŒ
    for agent_name, agent_factory in agents.items():
        print(f"\nğŸ“Š æµ‹è¯•ç­–ç•¥: {agent_name}")

        # å¤šæ¬¡è¿è¡Œæ±‚å¹³å‡
        all_rewards = np.zeros((n_runs, n_steps))
        all_optimal_actions = np.zeros((n_runs, n_steps))

        for run in range(n_runs):
            # åˆ›å»ºæ–°çš„Agentå’Œç¯å¢ƒ
            agent = agent_factory()
            run_bandit = MultiArmBandit(n_arms=10, seed=42+run)

            rewards = []
            optimal_actions = []

            for step in range(n_steps):
                # Agenté€‰æ‹©åŠ¨ä½œ
                action = agent.choose_action()

                # ç¯å¢ƒè¿”å›å¥–åŠ±
                reward = run_bandit.pull(action)

                # Agentæ›´æ–°çŸ¥è¯†
                agent.update(action, reward)

                # è®°å½•ç»“æœ
                rewards.append(reward)
                # ç”¨äºè®°å½•æ˜¯å¦é€‰æ‹©äº†æœ€ä¼˜çš„åŠ¨ä½œï¼Œç”¨äºè¯„ä¼°ç®—æ³•çš„æ€§èƒ½
                optimal_actions.append(1 if action == run_bandit.optimal_arm else 0)

            all_rewards[run] = rewards
            all_optimal_actions[run] = optimal_actions

        # è®¡ç®—å¹³å‡ç»“æœ
        # æ¯æ­¥çš„å¹³å‡å¥–åŠ±ï¼Œè¯„ä¼°å­¦ä¹ è¿›åº¦å’Œæœ€ç»ˆæ€§èƒ½
        avg_rewards = np.mean(all_rewards, axis=0)
        # æ¯æ­¥çš„æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡ï¼Œè¯„ä¼°å†³ç­–å‡†ç¡®æ€§å’Œå­¦ä¹ æ•ˆç‡
        avg_optimal_rate = np.mean(all_optimal_actions, axis=0)

        results[agent_name] = {
            'rewards': avg_rewards,
            'optimal_rate': avg_optimal_rate,
            'cumulative_reward': np.cumsum(avg_rewards),
            'final_reward': np.mean(avg_rewards[-100:]),  # æœ€å100æ­¥çš„å¹³å‡å¥–åŠ±
            'final_optimal_rate': np.mean(avg_optimal_rate[-100:])  # æœ€å100æ­¥çš„æœ€ä¼˜åŠ¨ä½œç‡
        }

        print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {results[agent_name]['final_reward']:.3f}")
        print(f"  æœ€ç»ˆæœ€ä¼˜åŠ¨ä½œç‡: {results[agent_name]['final_optimal_rate']:.1%}")

    # å¯è§†åŒ–ç»“æœ
    visualize_results(results, bandit.true_values, n_steps)

    # æ‰“å°æ€»ç»“
    print_summary(results)

def visualize_results(results: dict, true_values: np.ndarray, n_steps: int) -> None:
    """å¯è§†åŒ–å®éªŒç»“æœ"""

    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('å¤šè‡‚èµŒåšæœºå®éªŒç»“æœå¯¹æ¯”', fontsize=16, fontweight='bold')

    # 1. ç´¯ç§¯å¥–åŠ±å¯¹æ¯”
    ax1 = axes[0, 0]
    for agent_name, data in results.items():
        ax1.plot(data['cumulative_reward'], label=agent_name, linewidth=2)
    ax1.set_xlabel('æ—¶é—´æ­¥')
    ax1.set_ylabel('ç´¯ç§¯å¥–åŠ±')
    ax1.set_title('ç´¯ç§¯å¥–åŠ±å¯¹æ¯”')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. å¹³å‡å¥–åŠ±å¯¹æ¯”ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    ax2 = axes[0, 1]
    window_size = 100
    for agent_name, data in results.items():
        # è®¡ç®—æ»‘åŠ¨å¹³å‡
        rewards = data['rewards']
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        x = np.arange(window_size-1, len(rewards))
        ax2.plot(x, moving_avg, label=agent_name, linewidth=2)

    # æ·»åŠ æœ€ä¼˜æœŸæœ›å¥–åŠ±çº¿
    optimal_reward = np.max(true_values)
    ax2.axhline(y=optimal_reward, color='red', linestyle='--',
                label=f'æœ€ä¼˜æœŸæœ›å¥–åŠ± ({optimal_reward:.3f})', alpha=0.7)

    ax2.set_xlabel('æ—¶é—´æ­¥')
    ax2.set_ylabel('å¹³å‡å¥–åŠ±')
    ax2.set_title(f'å¹³å‡å¥–åŠ±å¯¹æ¯” (æ»‘åŠ¨çª—å£={window_size})')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡
    ax3 = axes[1, 0]
    for agent_name, data in results.items():
        # è®¡ç®—æ»‘åŠ¨å¹³å‡çš„æœ€ä¼˜åŠ¨ä½œç‡
        optimal_rate = data['optimal_rate']
        moving_avg = np.convolve(optimal_rate, np.ones(window_size)/window_size, mode='valid')
        x = np.arange(window_size-1, len(optimal_rate))
        ax3.plot(x, moving_avg, label=agent_name, linewidth=2)

    ax3.set_xlabel('æ—¶é—´æ­¥')
    ax3.set_ylabel('æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡')
    ax3.set_title(f'æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡ (æ»‘åŠ¨çª—å£={window_size})')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, 1)

    # 4. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    ax4 = axes[1, 1]
    agent_names = list(results.keys())
    final_rewards = [results[name]['final_reward'] for name in agent_names]
    final_optimal_rates = [results[name]['final_optimal_rate'] for name in agent_names]

    x = np.arange(len(agent_names))
    width = 0.35

    bars1 = ax4.bar(x - width/2, final_rewards, width, label='å¹³å‡å¥–åŠ±', alpha=0.8)
    bars2 = ax4.bar(x + width/2, final_optimal_rates, width, label='æœ€ä¼˜åŠ¨ä½œç‡', alpha=0.8)

    # æ·»åŠ æœ€ä¼˜å¥–åŠ±å‚è€ƒçº¿
    ax4.axhline(y=optimal_reward, color='red', linestyle='--', alpha=0.7)

    ax4.set_xlabel('ç­–ç•¥')
    ax4.set_ylabel('æ€§èƒ½')
    ax4.set_title('æœ€ç»ˆæ€§èƒ½å¯¹æ¯” (æœ€å100æ­¥å¹³å‡)')
    ax4.set_xticks(x)
    ax4.set_xticklabels(agent_names, rotation=45, ha='right')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, final_rewards):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom', fontsize=9)

    for bar, value in zip(bars2, final_optimal_rates):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.1%}', ha='center', va='bottom', fontsize=9)

    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case1_bandit_results.png',
                dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š ç»“æœå›¾å·²ä¿å­˜åˆ°: results/plots/case1_bandit_results.png")

    plt.show()

def print_summary(results: dict) -> None:
    """æ‰“å°å®éªŒæ€»ç»“"""
    print("\n" + "="*60)
    print("ğŸ¯ å®éªŒæ€»ç»“ï¼šæ¢ç´¢vsåˆ©ç”¨æƒè¡¡")
    print("="*60)

    # æŒ‰æœ€ç»ˆå¥–åŠ±æ’åº
    sorted_results = sorted(results.items(),
                           key=lambda x: x[1]['final_reward'],
                           reverse=True)

    print(f"{'ç­–ç•¥':<20} {'æœ€ç»ˆå¥–åŠ±':<12} {'æœ€ä¼˜åŠ¨ä½œç‡':<12} {'æ€»ç´¯ç§¯å¥–åŠ±':<15}")
    print("-" * 60)

    for agent_name, data in sorted_results:
        final_reward = data['final_reward']
        final_optimal_rate = data['final_optimal_rate']
        total_reward = data['cumulative_reward'][-1]

        print(f"{agent_name:<20} {final_reward:>8.3f}    {final_optimal_rate:>8.1%}    {total_reward:>10.1f}")

    print("\nğŸ’¡ å…³é”®æ´å¯Ÿï¼š")
    print("1. çº¯è´ªå¿ƒç­–ç•¥å¯èƒ½é™·å…¥æ¬¡ä¼˜è§£ï¼ˆå±€éƒ¨æœ€ä¼˜ï¼‰")
    print("2. é€‚åº¦æ¢ç´¢ï¼ˆÎµ=0.1ï¼‰é€šå¸¸æ¯”è¿‡åº¦æ¢ç´¢ï¼ˆÎµ=0.01ï¼‰æ•ˆæœæ›´å¥½")
    print("3. UCBç­–ç•¥èƒ½è‡ªé€‚åº”åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨")
    print("4. æ¢ç´¢çš„ä»·å€¼åœ¨é•¿æœŸè¿è¡Œä¸­ä½“ç°å¾—æ›´æ˜æ˜¾")

    print("\nğŸ” ç­–ç•¥ç‰¹ç‚¹ï¼š")
    print("â€¢ Îµ-è´ªå¿ƒï¼šç®€å•æœ‰æ•ˆï¼Œä½†æ¢ç´¢æ˜¯éšæœºçš„")
    print("â€¢ UCBï¼šæ™ºèƒ½æ¢ç´¢ï¼Œä¼˜å…ˆé€‰æ‹©ä¸ç¡®å®šæ€§é«˜çš„é€‰é¡¹")
    print("â€¢ çº¯è´ªå¿ƒï¼šæ”¶æ•›å¿«ä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜")

def interactive_demo() -> None:
    """äº¤äº’å¼æ¼”ç¤ºï¼Œè®©ç”¨æˆ·ä½“éªŒä¸åŒç­–ç•¥"""
    print("\nğŸ® äº¤äº’å¼ä½“éªŒï¼šä½ æ¥é€‰æ‹©ç­–ç•¥å‚æ•°ï¼")

    try:
        epsilon = float(input("è¯·è¾“å…¥Îµ-è´ªå¿ƒçš„æ¢ç´¢ç‡ (å»ºè®®0.01-0.2): "))
        c = float(input("è¯·è¾“å…¥UCBçš„ç½®ä¿¡åº¦å‚æ•° (å»ºè®®1.0-3.0): "))
        n_steps = int(input("è¯·è¾“å…¥å®éªŒæ­¥æ•° (å»ºè®®1000-5000): "))

        print(f"\nğŸ”§ ä½¿ç”¨å‚æ•°ï¼šÎµ={epsilon}, c={c}, æ­¥æ•°={n_steps}")

        # è¿è¡Œè‡ªå®šä¹‰å®éªŒ
        bandit = MultiArmBandit(n_arms=10, seed=42)

        agents = {
            f'ä½ çš„Îµ-è´ªå¿ƒ (Îµ={epsilon})': EpsilonGreedyAgent(10, epsilon=epsilon),
            f'ä½ çš„UCB (c={c})': UCBAgent(10, c=c),
            'åŸºçº¿Îµ-è´ªå¿ƒ (Îµ=0.1)': EpsilonGreedyAgent(10, epsilon=0.1),
            'åŸºçº¿UCB (c=2.0)': UCBAgent(10, c=2.0)
        }

        print(f"\nğŸƒ è¿è¡Œ{n_steps}æ­¥å®éªŒ...")

        for agent_name, agent in agents.items():
            total_reward = 0
            optimal_actions = 0

            for step in range(n_steps):
                action = agent.choose_action()
                reward = bandit.pull(action)
                agent.update(action, reward)

                total_reward += reward
                if action == bandit.optimal_arm:
                    optimal_actions += 1

            avg_reward = total_reward / n_steps
            optimal_rate = optimal_actions / n_steps

            print(f"{agent_name}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, æœ€ä¼˜ç‡={optimal_rate:.1%}")

    except (ValueError, KeyboardInterrupt):
        print("è¾“å…¥æ— æ•ˆæˆ–ç”¨æˆ·å–æ¶ˆï¼Œè·³è¿‡äº¤äº’å¼æ¼”ç¤º")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ° å¤šè‡‚èµŒåšæœºï¼šå¼ºåŒ–å­¦ä¹ å…¥é—¨æ¡ˆä¾‹")
    print("=" * 50)

    # è¿è¡Œæ ‡å‡†å®éªŒ
    run_experiment(n_steps=2000, n_runs=10)

    # äº¤äº’å¼æ¼”ç¤º
    # try:
    #     user_input = input("\næ˜¯å¦è¦å°è¯•äº¤äº’å¼æ¼”ç¤ºï¼Ÿ(y/n): ").strip().lower()
    #     if user_input == 'y':
    #         interactive_demo()
    # except KeyboardInterrupt:
    #     print("\nç”¨æˆ·å–æ¶ˆ")

    print("\nâœ… æ¡ˆä¾‹1å®Œæˆï¼")
    print("ğŸ“ ä½ å­¦åˆ°äº†ï¼š")
    print("  â€¢ æ¢ç´¢vsåˆ©ç”¨æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæƒè¡¡")
    print("  â€¢ ä¸åŒç­–ç•¥æœ‰ä¸åŒçš„æ¢ç´¢æ–¹å¼")
    print("  â€¢ é•¿æœŸæ€§èƒ½å¾€å¾€éœ€è¦çŸ­æœŸçš„æ¢ç´¢ä»£ä»·")
    print("\nâ¡ï¸  ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python case2_gridworld.py å­¦ä¹ Q-Learningç®—æ³•")

if __name__ == "__main__":
    main()