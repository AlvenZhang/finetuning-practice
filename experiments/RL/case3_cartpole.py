#!/usr/bin/env python3
"""
æ¡ˆä¾‹3ï¼šCartPole DQN - ä½“éªŒæ·±åº¦å¼ºåŒ–å­¦ä¹ 

è¿™æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç»å…¸å…¥é—¨é—®é¢˜ï¼š
- å¹³è¡¡æ†å­ä»»åŠ¡ï¼šé€šè¿‡å·¦å³ç§»åŠ¨å°è½¦æ¥ä¿æŒæ†å­ç›´ç«‹
- ä½¿ç”¨æ·±åº¦Qç½‘ç»œ(DQN)å¤„ç†è¿ç»­çŠ¶æ€ç©ºé—´
- ä½“éªŒç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œç­‰å…³é”®æŠ€æœ¯

è¿è¡Œæ—¶é—´ï¼šçº¦15åˆ†é’Ÿçœ‹åˆ°å­¦ä¹ æ•ˆæœ
å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£æ·±åº¦å­¦ä¹ åœ¨RLä¸­çš„ä½œç”¨
2. ä½“éªŒDQNçš„å…³é”®æŠ€æœ¯ï¼šç»éªŒå›æ”¾ã€ç›®æ ‡ç½‘ç»œ
3. è§‚å¯Ÿç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚æ§åˆ¶ä»»åŠ¡çš„è¿‡ç¨‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import random
from collections import deque
from typing import List, Tuple, Optional
import time

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# æ£€æµ‹è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else
                     "mps" if torch.backends.mps.is_available() else "cpu")
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

class DQN(nn.Module):
    """æ·±åº¦Qç½‘ç»œ

    ä½¿ç”¨å…¨è¿æ¥ç¥ç»ç½‘ç»œæ¥é€¼è¿‘Qå‡½æ•°
    è¾“å…¥ï¼šçŠ¶æ€ (4ç»´ï¼šä½ç½®, é€Ÿåº¦, è§’åº¦, è§’é€Ÿåº¦)
    è¾“å‡ºï¼šæ¯ä¸ªåŠ¨ä½œçš„Qå€¼ (2ç»´ï¼šå·¦ç§», å³ç§»)
    """

    def __init__(self, state_dim: int = 4, action_dim: int = 2, hidden_dim: int = 128):
        """åˆå§‹åŒ–ç½‘ç»œ

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(DQN, self).__init__()

        # å®šä¹‰ç½‘ç»œå±‚
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

        print(f"ğŸ§  åˆ›å»ºDQNç½‘ç»œ: {state_dim} â†’ {hidden_dim} â†’ {hidden_dim} â†’ {action_dim}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥çŠ¶æ€ [batch_size, state_dim]

        Returns:
            Qå€¼ [batch_size, action_dim]
        """
        return self.network(x)

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº

    å­˜å‚¨å’Œé‡‡æ ·å†å²ç»éªŒï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼Œæé«˜æ ·æœ¬æ•ˆç‡
    """

    def __init__(self, capacity: int = 10000):
        """åˆå§‹åŒ–ç¼“å†²åŒº

        Args:
            capacity: ç¼“å†²åŒºå®¹é‡
        """
        self.buffer = deque(maxlen=capacity)
        self.capacity = capacity

        print(f"ğŸ’¾ åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå®¹é‡: {capacity}")

    def push(self, state: np.ndarray, action: int, reward: float,
             next_state: np.ndarray, done: bool):
        """å­˜å‚¨ä¸€ä¸ªç»éªŒ

        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        experience = (state, action, reward, next_state, done)
        self.buffer.append(experience)

    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:
        """éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€çŠ¶æ€ã€ç»“æŸæ ‡å¿—çš„æ‰¹æ¬¡
        """
        batch = random.sample(self.buffer, batch_size)

        # åˆ†ç¦»å„ä¸ªç»„ä»¶
        states = np.array([e[0] for e in batch])
        actions = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])

        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.FloatTensor(states).to(device)
        actions = torch.LongTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        dones = torch.BoolTensor(dones).to(device)

        return states, actions, rewards, next_states, dones

    def __len__(self) -> int:
        return len(self.buffer)

class DQNAgent:
    """DQNæ™ºèƒ½ä½“

    å®ç°DQNç®—æ³•çš„æ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…æ‹¬ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ
    """

    def __init__(self,
                 state_dim: int = 4,
                 action_dim: int = 2,
                 learning_rate: float = 1e-3,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_min: float = 0.01,
                 epsilon_decay: float = 0.995,
                 buffer_size: int = 10000,
                 batch_size: int = 32,
                 target_update_freq: int = 100):
        """åˆå§‹åŒ–DQN Agent

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon: åˆå§‹æ¢ç´¢ç‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            buffer_size: ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
            batch_size: æ‰¹æ¬¡å¤§å°
            target_update_freq: ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq

        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = DQN(state_dim, action_dim).to(device)
        self.target_network = DQN(state_dim, action_dim).to(device)

        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œï¼ˆå¤åˆ¶ä¸»ç½‘ç»œå‚æ•°ï¼‰
        self.update_target_network()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = ReplayBuffer(buffer_size)

        # è®­ç»ƒè®¡æ•°å™¨
        self.train_step = 0

        print(f"ğŸ¤– åˆ›å»ºDQN Agent")
        print(f"å‚æ•°: lr={learning_rate}, Î³={gamma}, Îµ={epsilon}â†’{epsilon_min}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡: {target_update_freq}")

    def choose_action(self, state: np.ndarray, training: bool = True) -> int:
        """é€‰æ‹©åŠ¨ä½œ

        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼

        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randint(0, self.action_dim - 1)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()

    def store_experience(self, state: np.ndarray, action: int, reward: float,
                        next_state: np.ndarray, done: bool):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.push(state, action, reward, next_state, done)

    def train(self) -> Optional[float]:
        """è®­ç»ƒç½‘ç»œ

        Returns:
            æŸå¤±å€¼ï¼ˆå¦‚æœè¿›è¡Œäº†è®­ç»ƒï¼‰
        """
        # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰è¶³å¤Ÿçš„ç»éªŒ
        if len(self.memory) < self.batch_size:
            return None

        # ä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values, target_q_values)

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)

        self.optimizer.step()

        # æ›´æ–°è®­ç»ƒè®¡æ•°å™¨
        self.train_step += 1

        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.train_step % self.target_update_freq == 0:
            self.update_target_network()

        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        return loss.item()

    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆå¤åˆ¶ä¸»ç½‘ç»œå‚æ•°ï¼‰"""
        self.target_network.load_state_dict(self.q_network.state_dict())

    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'train_step': self.train_step
        }, filepath)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.train_step = checkpoint['train_step']
        print(f"ğŸ“‚ æ¨¡å‹å·²ä» {filepath} åŠ è½½")

def run_training(episodes: int = 1000, max_steps: int = 500) -> Tuple[DQNAgent, List[float], List[float]]:
    """è¿è¡ŒDQNè®­ç»ƒ

    Args:
        episodes: è®­ç»ƒå›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°

    Returns:
        è®­ç»ƒå¥½çš„Agentã€å¥–åŠ±å†å²ã€æŸå¤±å†å²
    """
    print(f"\nğŸš€ å¼€å§‹DQNè®­ç»ƒ")
    print(f"å‚æ•°ï¼š{episodes}ä¸ªå›åˆï¼Œæ¯å›åˆæœ€å¤š{max_steps}æ­¥")

    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    print(f"ğŸ® CartPoleç¯å¢ƒ: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")

    # åˆ›å»ºAgent
    agent = DQNAgent(state_dim=state_dim, action_dim=action_dim)

    # è®°å½•è®­ç»ƒæ•°æ®
    episode_rewards = []
    episode_lengths = []
    losses = []
    epsilon_history = []

    # ç”¨äºå®æ—¶æ˜¾ç¤ºçš„å˜é‡
    recent_rewards = deque(maxlen=100)  # æœ€è¿‘100å›åˆçš„å¥–åŠ±
    best_avg_reward = -float('inf')

    print(f"\nğŸ“Š å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()

    for episode in range(episodes):
        # é‡ç½®ç¯å¢ƒ
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        episode_losses = []

        while steps < max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.choose_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # å­˜å‚¨ç»éªŒ
            agent.store_experience(state, action, reward, next_state, done)

            # è®­ç»ƒç½‘ç»œ
            loss = agent.train()
            if loss is not None:
                episode_losses.append(loss)

            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            total_reward += reward
            steps += 1

            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if done:
                break

        # è®°å½•å›åˆæ•°æ®
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        recent_rewards.append(total_reward)
        epsilon_history.append(agent.epsilon)

        if episode_losses:
            losses.append(np.mean(episode_losses))
        else:
            losses.append(0)

        # è®¡ç®—æœ€è¿‘100å›åˆçš„å¹³å‡å¥–åŠ±
        avg_reward = np.mean(recent_rewards)

        # å®šæœŸæ‰“å°è¿›åº¦
        if (episode + 1) % 50 == 0:
            elapsed_time = time.time() - start_time
            print(f"å›åˆ {episode+1:4d}: å¥–åŠ±={total_reward:6.1f}, "
                  f"å¹³å‡å¥–åŠ±={avg_reward:6.1f}, æ­¥æ•°={steps:3d}, "
                  f"Îµ={agent.epsilon:.3f}, æ—¶é—´={elapsed_time:.1f}s")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_reward > best_avg_reward and len(recent_rewards) >= 100:
            best_avg_reward = avg_reward
            agent.save_model('/Users/xifeng/project/finetuning-0106/experiments/RL/results/models/best_cartpole_dqn.pth')

        # æ—©åœæ¡ä»¶ï¼šè¿ç»­100å›åˆå¹³å‡å¥–åŠ± >= 475ï¼ˆæ¥è¿‘æœ€å¤§å€¼500ï¼‰
        if len(recent_rewards) >= 100 and avg_reward >= 475:
            print(f"\nğŸ‰ æå‰è¾¾åˆ°ç›®æ ‡ï¼å¹³å‡å¥–åŠ±: {avg_reward:.1f}")
            break

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model('/Users/xifeng/project/finetuning-0106/experiments/RL/results/models/final_cartpole_dqn.pth')

    total_time = time.time() - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"æ€»æ—¶é—´: {total_time:.1f}ç§’")
    print(f"æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.3f}")
    print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.1f}")

    env.close()
    return agent, episode_rewards, losses, epsilon_history

def test_agent(agent: DQNAgent, n_tests: int = 5, render: bool = False) -> None:
    """æµ‹è¯•è®­ç»ƒå¥½çš„Agent

    Args:
        agent: è®­ç»ƒå¥½çš„Agent
        n_tests: æµ‹è¯•æ¬¡æ•°
        render: æ˜¯å¦æ¸²æŸ“ç¯å¢ƒ
    """
    print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„Agent ({n_tests}æ¬¡æµ‹è¯•)")

    # åˆ›å»ºç¯å¢ƒï¼ˆå¯é€‰æ‹©æ¸²æŸ“ï¼‰
    if render:
        env = gym.make('CartPole-v1', render_mode='human')
    else:
        env = gym.make('CartPole-v1')

    test_rewards = []

    for test in range(n_tests):
        state, _ = env.reset()
        total_reward = 0
        steps = 0

        print(f"\næµ‹è¯• {test+1}:")

        while True:
            # ä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.choose_action(state, training=False)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            total_reward += reward
            steps += 1
            state = next_state

            # å¯é€‰ï¼šæ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
            if not render and steps % 50 == 0:
                print(f"  æ­¥éª¤ {steps}: ä½ç½®={state[0]:.3f}, è§’åº¦={state[2]:.3f}")

            if done:
                break

            # é˜²æ­¢æ— é™å¾ªç¯
            if steps >= 500:
                break

        test_rewards.append(total_reward)
        print(f"  æ€»å¥–åŠ±: {total_reward}, æ­¥æ•°: {steps}")

        if render:
            time.sleep(1)  # æš‚åœä»¥ä¾¿è§‚å¯Ÿ

    # ç»Ÿè®¡ç»“æœ
    avg_reward = np.mean(test_rewards)
    std_reward = np.std(test_rewards)

    print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
    print(f"å¹³å‡å¥–åŠ±: {avg_reward:.1f} Â± {std_reward:.1f}")
    print(f"æœ€ä½³è¡¨ç°: {max(test_rewards):.1f}")
    print(f"æœ€å·®è¡¨ç°: {min(test_rewards):.1f}")

    # CartPoleçš„è¯„ä»·æ ‡å‡†
    if avg_reward >= 475:
        print("ğŸ† ä¼˜ç§€ï¼è¾¾åˆ°äº†CartPoleçš„è§£å†³æ ‡å‡†ï¼ˆå¹³å‡å¥–åŠ±â‰¥475ï¼‰")
    elif avg_reward >= 400:
        print("ğŸ‘ è‰¯å¥½ï¼æ¥è¿‘è§£å†³æ ‡å‡†")
    elif avg_reward >= 200:
        print("ğŸ“ˆ ä¸é”™ï¼æœ‰æ˜æ˜¾å­¦ä¹ æ•ˆæœ")
    else:
        print("ğŸ“š è¿˜éœ€è¦æ›´å¤šè®­ç»ƒ")

    env.close()

def visualize_training_results(episode_rewards: List[float],
                             losses: List[float],
                             epsilon_history: List[float]) -> None:
    """å¯è§†åŒ–è®­ç»ƒç»“æœ"""

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('DQNè®­ç»ƒè¿‡ç¨‹åˆ†æ', fontsize=16, fontweight='bold')

    episodes = range(len(episode_rewards))

    # 1. å›åˆå¥–åŠ±
    ax1 = axes[0, 0]
    ax1.plot(episodes, episode_rewards, alpha=0.6, linewidth=0.8, color='blue')

    # æ·»åŠ æ»‘åŠ¨å¹³å‡
    window_size = 50
    if len(episode_rewards) > window_size:
        moving_avg = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')
        ax1.plot(range(window_size-1, len(episode_rewards)), moving_avg,
                'r-', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size})')
        ax1.legend()

    # æ·»åŠ ç›®æ ‡çº¿
    ax1.axhline(y=475, color='green', linestyle='--', alpha=0.7, label='ç›®æ ‡(475)')
    ax1.legend()

    ax1.set_xlabel('å›åˆ')
    ax1.set_ylabel('æ€»å¥–åŠ±')
    ax1.set_title('å›åˆå¥–åŠ±å˜åŒ–')
    ax1.grid(True, alpha=0.3)

    # 2. æŸå¤±å˜åŒ–
    ax2 = axes[0, 1]
    if losses and max(losses) > 0:  # ç¡®ä¿æœ‰æœ‰æ•ˆçš„æŸå¤±æ•°æ®
        ax2.plot(episodes, losses, alpha=0.6, linewidth=0.8, color='orange')

        # æ·»åŠ æ»‘åŠ¨å¹³å‡
        if len(losses) > window_size:
            moving_avg = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')
            ax2.plot(range(window_size-1, len(losses)), moving_avg,
                    'r-', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size})')
            ax2.legend()
    else:
        ax2.text(0.5, 0.5, 'æš‚æ— æŸå¤±æ•°æ®', ha='center', va='center',
                transform=ax2.transAxes, fontsize=12)

    ax2.set_xlabel('å›åˆ')
    ax2.set_ylabel('æŸå¤±')
    ax2.set_title('è®­ç»ƒæŸå¤±å˜åŒ–')
    ax2.grid(True, alpha=0.3)

    # 3. æ¢ç´¢ç‡è¡°å‡
    ax3 = axes[1, 0]
    ax3.plot(episodes, epsilon_history, color='purple', linewidth=2)
    ax3.set_xlabel('å›åˆ')
    ax3.set_ylabel('æ¢ç´¢ç‡ (Îµ)')
    ax3.set_title('æ¢ç´¢ç‡è¡°å‡')
    ax3.grid(True, alpha=0.3)

    # 4. æ€§èƒ½åˆ†å¸ƒ
    ax4 = axes[1, 1]
    ax4.hist(episode_rewards, bins=30, alpha=0.7, edgecolor='black', color='skyblue')
    ax4.axvline(np.mean(episode_rewards), color='red', linestyle='--',
               label=f'å¹³å‡å€¼: {np.mean(episode_rewards):.1f}')
    ax4.axvline(475, color='green', linestyle='--', alpha=0.7, label='ç›®æ ‡: 475')
    ax4.legend()
    ax4.set_xlabel('æ€»å¥–åŠ±')
    ax4.set_ylabel('é¢‘æ¬¡')
    ax4.set_title('å¥–åŠ±åˆ†å¸ƒ')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case3_training_results.png',
                dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š è®­ç»ƒç»“æœå›¾å·²ä¿å­˜åˆ°: results/plots/case3_training_results.png")

    plt.show()

def compare_with_random_policy() -> None:
    """ä¸éšæœºç­–ç•¥å¯¹æ¯”"""
    print(f"\nğŸ² ä¸éšæœºç­–ç•¥å¯¹æ¯”")

    env = gym.make('CartPole-v1')
    n_tests = 10

    # æµ‹è¯•éšæœºç­–ç•¥
    random_rewards = []
    for _ in range(n_tests):
        state, _ = env.reset()
        total_reward = 0

        while True:
            action = env.action_space.sample()  # éšæœºåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward

            if terminated or truncated:
                break

        random_rewards.append(total_reward)

    random_avg = np.mean(random_rewards)
    random_std = np.std(random_rewards)

    print(f"éšæœºç­–ç•¥å¹³å‡å¥–åŠ±: {random_avg:.1f} Â± {random_std:.1f}")

    # å¦‚æœæœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œå¯¹æ¯”
    try:
        agent = DQNAgent()
        agent.load_model('/Users/xifeng/project/finetuning-0106/experiments/RL/results/models/best_cartpole_dqn.pth')

        # æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥
        dqn_rewards = []
        for _ in range(n_tests):
            state, _ = env.reset()
            total_reward = 0

            while True:
                action = agent.choose_action(state, training=False)
                next_state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                state = next_state

                if terminated or truncated:
                    break

            dqn_rewards.append(total_reward)

        dqn_avg = np.mean(dqn_rewards)
        dqn_std = np.std(dqn_rewards)

        print(f"DQNç­–ç•¥å¹³å‡å¥–åŠ±: {dqn_avg:.1f} Â± {dqn_std:.1f}")
        print(f"æ€§èƒ½æå‡: {dqn_avg - random_avg:.1f} (+{(dqn_avg/random_avg-1)*100:.1f}%)")

        # å¯è§†åŒ–å¯¹æ¯”
        plt.figure(figsize=(10, 6))

        x = ['éšæœºç­–ç•¥', 'DQNç­–ç•¥']
        means = [random_avg, dqn_avg]
        stds = [random_std, dqn_std]

        bars = plt.bar(x, means, yerr=stds, capsize=5, alpha=0.8,
                      color=['red', 'blue'], edgecolor='black')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean, std in zip(bars, means, stds):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 5,
                    f'{mean:.1f}Â±{std:.1f}', ha='center', va='bottom', fontweight='bold')

        plt.ylabel('å¹³å‡å¥–åŠ±')
        plt.title('ç­–ç•¥æ€§èƒ½å¯¹æ¯”')
        plt.grid(True, alpha=0.3)

        # æ·»åŠ ç›®æ ‡çº¿
        plt.axhline(y=475, color='green', linestyle='--', alpha=0.7, label='ç›®æ ‡(475)')
        plt.legend()

        plt.tight_layout()
        plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case3_policy_comparison.png',
                   dpi=300, bbox_inches='tight')
        plt.show()

    except FileNotFoundError:
        print("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆå®Œæˆè®­ç»ƒ")

    env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– CartPole DQNï¼šæ·±åº¦å¼ºåŒ–å­¦ä¹ å…¥é—¨")
    print("=" * 45)

    # è¿è¡Œè®­ç»ƒ
    agent, episode_rewards, losses, epsilon_history = run_training(
        episodes=1000, max_steps=500
    )

    # å¯è§†åŒ–è®­ç»ƒç»“æœ
    visualize_training_results(episode_rewards, losses, epsilon_history)

    # æµ‹è¯•Agent
    test_agent(agent, n_tests=5, render=False)

    # ä¸éšæœºç­–ç•¥å¯¹æ¯”
    compare_with_random_policy()

    # è¯¢é—®æ˜¯å¦è§‚çœ‹å®æ—¶æ¼”ç¤º
    try:
        user_input = input("\næ˜¯å¦è¦è§‚çœ‹å®æ—¶æ¼”ç¤ºï¼Ÿ(y/n): ").strip().lower()
        if user_input == 'y':
            print("ğŸ¬ å¯åŠ¨å®æ—¶æ¼”ç¤º...")
            test_agent(agent, n_tests=3, render=True)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·å–æ¶ˆ")

    print("\nâœ… æ¡ˆä¾‹3å®Œæˆï¼")
    print("ğŸ“ ä½ å­¦åˆ°äº†ï¼š")
    print("  â€¢ ç¥ç»ç½‘ç»œå¦‚ä½•é€¼è¿‘å¤æ‚çš„ä»·å€¼å‡½æ•°")
    print("  â€¢ ç»éªŒå›æ”¾å¦‚ä½•æé«˜æ ·æœ¬æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§")
    print("  â€¢ ç›®æ ‡ç½‘ç»œå¦‚ä½•è§£å†³è®­ç»ƒä¸ç¨³å®šé—®é¢˜")
    print("  â€¢ DQNç›¸æ¯”éšæœºç­–ç•¥çš„å·¨å¤§æ€§èƒ½æå‡")
    print("\nâ¡ï¸  ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python case4_comparison.py å¯¹æ¯”æ‰€æœ‰ç®—æ³•")

if __name__ == "__main__":
    main()