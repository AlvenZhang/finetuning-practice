#!/usr/bin/env python3
"""
æ¡ˆä¾‹2ï¼šç½‘æ ¼ä¸–ç•ŒQ-Learning - ç†è§£ä»·å€¼å‡½æ•°å’Œç­–ç•¥å­¦ä¹ 

è¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„ç»å…¸é—®é¢˜ï¼š
- 5x5ç½‘æ ¼ä¸–ç•Œï¼ŒAgentä»èµ·ç‚¹èµ°åˆ°ç»ˆç‚¹
- å­¦ä¹ æœ€ä¼˜è·¯å¾„ï¼Œé¿å¼€éšœç¢ç‰©
- ä½¿ç”¨Q-Learningç®—æ³•å­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•°

è¿è¡Œæ—¶é—´ï¼šçº¦10åˆ†é’Ÿçœ‹åˆ°å­¦ä¹ è¿‡ç¨‹
å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£Q-Learningç®—æ³•çš„æ ¸å¿ƒæ€æƒ³
2. è§‚å¯Ÿä»·å€¼å‡½æ•°çš„å­¦ä¹ è¿‡ç¨‹
3. ç†è§£ç­–ç•¥å¦‚ä½•ä»ä»·å€¼å‡½æ•°ä¸­æå–
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, List, Dict
import time

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class GridWorld:
    """5x5ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ

    Agentéœ€è¦ä»èµ·ç‚¹(0,0)èµ°åˆ°ç»ˆç‚¹(4,4)ï¼Œé¿å¼€éšœç¢ç‰©
    """

    def __init__(self, size: int = 5):
        """åˆå§‹åŒ–ç½‘æ ¼ä¸–ç•Œ

        Args:
            size: ç½‘æ ¼å¤§å°
        """
        self.size = size
        self.start_pos = (0, 0)
        self.goal_pos = (4, 4)

        # è®¾ç½®éšœç¢ç‰©ä½ç½®
        self.obstacles = {(1, 1), (1, 2), (2, 1), (3, 3)}

        # åŠ¨ä½œï¼šä¸Šã€ä¸‹ã€å·¦ã€å³
        self.actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]
        self.action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
        self.n_actions = len(self.actions)

        # å½“å‰çŠ¶æ€
        self.current_pos = self.start_pos

        print(f"ğŸ—ºï¸  åˆ›å»º{size}x{size}ç½‘æ ¼ä¸–ç•Œ")
        print(f"èµ·ç‚¹: {self.start_pos}, ç»ˆç‚¹: {self.goal_pos}")
        print(f"éšœç¢ç‰©: {self.obstacles}")

    def reset(self) -> Tuple[int, int]:
        """é‡ç½®ç¯å¢ƒåˆ°èµ·å§‹çŠ¶æ€"""
        self.current_pos = self.start_pos
        return self.current_pos

    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:
        """æ‰§è¡ŒåŠ¨ä½œ

        Args:
            action: åŠ¨ä½œç¼–å· (0:ä¸Š, 1:ä¸‹, 2:å·¦, 3:å³)

        Returns:
            ä¸‹ä¸€ä¸ªçŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»“æŸ
        """
        # è®¡ç®—ä¸‹ä¸€ä¸ªä½ç½®
        dx, dy = self.actions[action]
        next_x = self.current_pos[0] + dx
        next_y = self.current_pos[1] + dy
        next_pos = (next_x, next_y)

        # æ£€æŸ¥è¾¹ç•Œ
        if (next_x < 0 or next_x >= self.size or
            next_y < 0 or next_y >= self.size):
            # æ’å¢™ï¼Œä½ç½®ä¸å˜ï¼Œç»™äºˆè´Ÿå¥–åŠ±
            reward = -0.1
            done = False
            return self.current_pos, reward, done

        # æ£€æŸ¥éšœç¢ç‰©
        if next_pos in self.obstacles:
            # æ’åˆ°éšœç¢ç‰©ï¼Œä½ç½®ä¸å˜ï¼Œç»™äºˆè´Ÿå¥–åŠ±
            reward = -0.5
            done = False
            return self.current_pos, reward, done

        # æ­£å¸¸ç§»åŠ¨
        self.current_pos = next_pos

        # è®¡ç®—å¥–åŠ±
        if next_pos == self.goal_pos:
            reward = 10.0  # åˆ°è¾¾ç›®æ ‡ï¼Œå¤§å¥–åŠ±
            done = True
        else:
            reward = -0.01  # æ¯æ­¥å°æƒ©ç½šï¼Œé¼“åŠ±æ‰¾æœ€çŸ­è·¯å¾„
            done = False

        return self.current_pos, reward, done

    def get_state_id(self, pos: Tuple[int, int]) -> int:
        """å°†äºŒç»´åæ ‡è½¬æ¢ä¸ºçŠ¶æ€ID"""
        return pos[1] * self.size + pos[0]

    def get_pos_from_id(self, state_id: int) -> Tuple[int, int]:
        """å°†çŠ¶æ€IDè½¬æ¢ä¸ºäºŒç»´åæ ‡"""
        x = state_id % self.size
        y = state_id // self.size
        return (x, y)

    def visualize(self, q_table: np.ndarray = None, policy: np.ndarray = None):
        """å¯è§†åŒ–ç½‘æ ¼ä¸–ç•Œ

        Args:
            q_table: Qè¡¨ï¼Œç”¨äºæ˜¾ç¤ºä»·å€¼å‡½æ•°
            policy: ç­–ç•¥ï¼Œç”¨äºæ˜¾ç¤ºæœ€ä¼˜åŠ¨ä½œ
        """
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        # 1. ç¯å¢ƒå¸ƒå±€
        ax1 = axes[0]
        grid = np.zeros((self.size, self.size))

        # è®¾ç½®éšœç¢ç‰©
        for obs in self.obstacles:
            grid[obs[1], obs[0]] = -1

        # è®¾ç½®èµ·ç‚¹å’Œç»ˆç‚¹
        grid[self.start_pos[1], self.start_pos[0]] = 0.5
        grid[self.goal_pos[1], self.goal_pos[0]] = 1

        im1 = ax1.imshow(grid, cmap='RdYlGn', vmin=-1, vmax=1)
        ax1.set_title('ç¯å¢ƒå¸ƒå±€', fontsize=14, fontweight='bold')

        # æ·»åŠ æ–‡å­—æ ‡æ³¨
        for y in range(self.size):
            for x in range(self.size):
                if (x, y) == self.start_pos:
                    ax1.text(x, y, 'S', ha='center', va='center',
                            fontsize=12, fontweight='bold', color='blue')
                elif (x, y) == self.goal_pos:
                    ax1.text(x, y, 'G', ha='center', va='center',
                            fontsize=12, fontweight='bold', color='red')
                elif (x, y) in self.obstacles:
                    ax1.text(x, y, 'â– ', ha='center', va='center',
                            fontsize=12, color='black')

        ax1.set_xticks(range(self.size))
        ax1.set_yticks(range(self.size))
        ax1.grid(True, alpha=0.3)

        # 2. çŠ¶æ€ä»·å€¼å‡½æ•° (å¦‚æœæä¾›äº†Qè¡¨)
        ax2 = axes[1]
        if q_table is not None:
            # è®¡ç®—çŠ¶æ€ä»·å€¼ï¼šV(s) = max_a Q(s,a)
            state_values = np.zeros((self.size, self.size))
            for y in range(self.size):
                for x in range(self.size):
                    if (x, y) not in self.obstacles:
                        state_id = self.get_state_id((x, y))
                        state_values[y, x] = np.max(q_table[state_id])
                    else:
                        state_values[y, x] = np.nan

            im2 = ax2.imshow(state_values, cmap='viridis')
            plt.colorbar(im2, ax=ax2, shrink=0.8)
            ax2.set_title('çŠ¶æ€ä»·å€¼å‡½æ•° V(s)', fontsize=14, fontweight='bold')

            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for y in range(self.size):
                for x in range(self.size):
                    if (x, y) not in self.obstacles:
                        value = state_values[y, x]
                        ax2.text(x, y, f'{value:.2f}', ha='center', va='center',
                                fontsize=10, color='white', fontweight='bold')
        else:
            ax2.text(0.5, 0.5, 'ç­‰å¾…Qè¡¨æ•°æ®...', ha='center', va='center',
                    transform=ax2.transAxes, fontsize=14)
            ax2.set_title('çŠ¶æ€ä»·å€¼å‡½æ•°', fontsize=14)

        ax2.set_xticks(range(self.size))
        ax2.set_yticks(range(self.size))
        ax2.grid(True, alpha=0.3)

        # 3. æœ€ä¼˜ç­–ç•¥ (å¦‚æœæä¾›äº†ç­–ç•¥)
        ax3 = axes[2]
        if policy is not None:
            # åˆ›å»ºç­–ç•¥å¯è§†åŒ–
            policy_grid = np.zeros((self.size, self.size))
            arrow_symbols = ['â†‘', 'â†“', 'â†', 'â†’']

            for y in range(self.size):
                for x in range(self.size):
                    if (x, y) not in self.obstacles and (x, y) != self.goal_pos:
                        state_id = self.get_state_id((x, y))
                        best_action = policy[state_id]
                        ax3.text(x, y, arrow_symbols[best_action], ha='center', va='center',
                                fontsize=16, fontweight='bold', color='blue')

            # è®¾ç½®èƒŒæ™¯
            background = np.ones((self.size, self.size))
            for obs in self.obstacles:
                background[obs[1], obs[0]] = 0.5

            ax3.imshow(background, cmap='gray', alpha=0.3)
            ax3.set_title('æœ€ä¼˜ç­–ç•¥ Ï€*(s)', fontsize=14, fontweight='bold')

            # æ ‡æ³¨èµ·ç‚¹å’Œç»ˆç‚¹
            ax3.text(self.start_pos[0], self.start_pos[1], 'S',
                    ha='center', va='center', fontsize=12, fontweight='bold',
                    color='green', bbox=dict(boxstyle='circle', facecolor='white'))
            ax3.text(self.goal_pos[0], self.goal_pos[1], 'G',
                    ha='center', va='center', fontsize=12, fontweight='bold',
                    color='red', bbox=dict(boxstyle='circle', facecolor='white'))
        else:
            ax3.text(0.5, 0.5, 'ç­‰å¾…ç­–ç•¥æ•°æ®...', ha='center', va='center',
                    transform=ax3.transAxes, fontsize=14)
            ax3.set_title('æœ€ä¼˜ç­–ç•¥', fontsize=14)

        ax3.set_xticks(range(self.size))
        ax3.set_yticks(range(self.size))
        ax3.grid(True, alpha=0.3)

        plt.tight_layout()
        return fig

class QLearningAgent:
    """Q-Learningç®—æ³•å®ç°

    å­¦ä¹ åŠ¨ä½œä»·å€¼å‡½æ•°Q(s,a)ï¼Œå¹¶ä»ä¸­æå–æœ€ä¼˜ç­–ç•¥
    """

    def __init__(self, n_states: int, n_actions: int,
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.95,
                 epsilon: float = 0.1):
        """åˆå§‹åŒ–Q-Learning Agent

        Args:
            n_states: çŠ¶æ€æ•°é‡
            n_actions: åŠ¨ä½œæ•°é‡
            learning_rate: å­¦ä¹ ç‡Î±
            discount_factor: æŠ˜æ‰£å› å­Î³
            epsilon: æ¢ç´¢ç‡Îµ
        """
        self.n_states = n_states
        self.n_actions = n_actions
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon

        # åˆå§‹åŒ–Qè¡¨
        self.q_table = np.zeros((n_states, n_actions))

        print(f"ğŸ¤– åˆ›å»ºQ-Learning Agent")
        print(f"å‚æ•°: Î±={learning_rate}, Î³={discount_factor}, Îµ={epsilon}")

    def choose_action(self, state: int, training: bool = True) -> int:
        """æ ¹æ®Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ

        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨Îµ-è´ªå¿ƒï¼Œæµ‹è¯•æ—¶ä½¿ç”¨è´ªå¿ƒï¼‰

        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return np.random.randint(self.n_actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            return np.argmax(self.q_table[state])

    def update(self, state: int, action: int, reward: float,
               next_state: int, done: bool) -> float:
        """æ›´æ–°Qè¡¨

        Q-Learningæ›´æ–°è§„åˆ™ï¼š
        Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]

        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            done: æ˜¯å¦ç»“æŸ

        Returns:
            TDè¯¯å·®ï¼ˆç”¨äºç›‘æ§å­¦ä¹ è¿›åº¦ï¼‰
        """
        # è®¡ç®—ç›®æ ‡å€¼
        if done:
            target = reward
        else:
            target = reward + self.gamma * np.max(self.q_table[next_state])

        # è®¡ç®—TDè¯¯å·®
        td_error = target - self.q_table[state, action]

        # æ›´æ–°Qå€¼
        self.q_table[state, action] += self.lr * td_error

        return abs(td_error)

    def get_policy(self) -> np.ndarray:
        """ä»Qè¡¨æå–è´ªå¿ƒç­–ç•¥

        Returns:
            ç­–ç•¥æ•°ç»„ï¼Œpolicy[s] = argmax_a Q(s,a)
        """
        return np.argmax(self.q_table, axis=1)

    def decay_epsilon(self, decay_rate: float = 0.995):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(0.01, self.epsilon * decay_rate)

def run_training(episodes: int = 1000, visualize_every: int = 200) -> Tuple[QLearningAgent, List[float]]:
    """è¿è¡ŒQ-Learningè®­ç»ƒ

    Args:
        episodes: è®­ç»ƒå›åˆæ•°
        visualize_every: æ¯éš”å¤šå°‘å›åˆå¯è§†åŒ–ä¸€æ¬¡

    Returns:
        è®­ç»ƒå¥½çš„Agentå’Œå¥–åŠ±å†å²
    """
    print(f"\nğŸš€ å¼€å§‹Q-Learningè®­ç»ƒ")
    print(f"å‚æ•°ï¼š{episodes}ä¸ªå›åˆ")

    # åˆ›å»ºç¯å¢ƒå’ŒAgent
    env = GridWorld(size=5)
    n_states = env.size * env.size
    agent = QLearningAgent(n_states, env.n_actions,
                          learning_rate=0.1,
                          discount_factor=0.95,
                          epsilon=0.3)  # å¼€å§‹æ—¶è¾ƒé«˜çš„æ¢ç´¢ç‡

    # è®°å½•è®­ç»ƒæ•°æ®
    episode_rewards = []
    episode_lengths = []
    td_errors = []

    print(f"\nğŸ“Š å¼€å§‹è®­ç»ƒ...")

    for episode in range(episodes):
        # é‡ç½®ç¯å¢ƒ
        pos = env.reset()
        state = env.get_state_id(pos)
        total_reward = 0
        steps = 0
        episode_td_errors = []

        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.choose_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_pos, reward, done = env.step(action)
            next_state = env.get_state_id(next_pos)

            # æ›´æ–°Qè¡¨
            td_error = agent.update(state, action, reward, next_state, done)

            # è®°å½•æ•°æ®
            total_reward += reward
            steps += 1
            episode_td_errors.append(td_error)

            # æ›´æ–°çŠ¶æ€
            state = next_state

            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if done or steps > 100:  # é˜²æ­¢æ— é™å¾ªç¯
                break

        # è®°å½•å›åˆæ•°æ®
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        td_errors.append(np.mean(episode_td_errors))

        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon(0.995)

        # å®šæœŸæ‰“å°è¿›åº¦å’Œå¯è§†åŒ–
        if (episode + 1) % visualize_every == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            current_epsilon = agent.epsilon

            print(f"å›åˆ {episode+1:4d}: å¹³å‡å¥–åŠ±={avg_reward:6.2f}, "
                  f"å¹³å‡æ­¥æ•°={avg_length:5.1f}, Îµ={current_epsilon:.3f}")

            # å¯è§†åŒ–å½“å‰çŠ¶æ€
            if episode >= visualize_every - 1:  # ä»ç¬¬ä¸€æ¬¡å¯è§†åŒ–å¼€å§‹
                policy = agent.get_policy()
                fig = env.visualize(agent.q_table, policy)
                fig.suptitle(f'Q-Learningè®­ç»ƒè¿›åº¦ - å›åˆ {episode+1}', fontsize=16, fontweight='bold')

                # ä¿å­˜å›¾ç‰‡
                plt.savefig(f'/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case2_episode_{episode+1}.png',
                           dpi=150, bbox_inches='tight')
                plt.show()
                time.sleep(1)  # çŸ­æš‚æš‚åœä»¥ä¾¿è§‚å¯Ÿ

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.3f}")

    return agent, episode_rewards, episode_lengths, td_errors

def test_agent(agent: QLearningAgent, env: GridWorld, n_tests: int = 5) -> None:
    """æµ‹è¯•è®­ç»ƒå¥½çš„Agent

    Args:
        agent: è®­ç»ƒå¥½çš„Agent
        env: ç¯å¢ƒ
        n_tests: æµ‹è¯•æ¬¡æ•°
    """
    print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„Agent ({n_tests}æ¬¡æµ‹è¯•)")

    success_count = 0
    total_steps = []

    for test in range(n_tests):
        pos = env.reset()
        state = env.get_state_id(pos)
        path = [pos]
        steps = 0

        print(f"\næµ‹è¯• {test+1}: èµ·ç‚¹{pos} â†’ ç»ˆç‚¹{env.goal_pos}")

        while True:
            # ä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.choose_action(state, training=False)
            action_name = env.action_names[action]

            # æ‰§è¡ŒåŠ¨ä½œ
            next_pos, reward, done = env.step(action)
            next_state = env.get_state_id(next_pos)

            path.append(next_pos)
            steps += 1

            print(f"  æ­¥éª¤{steps}: {env.current_pos} --{action_name}--> {next_pos} (å¥–åŠ±: {reward:.2f})")

            # æ›´æ–°çŠ¶æ€
            state = next_state

            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if done:
                print(f"  âœ… æˆåŠŸåˆ°è¾¾ç›®æ ‡ï¼æ€»æ­¥æ•°: {steps}")
                success_count += 1
                total_steps.append(steps)
                break
            elif steps > 50:  # é˜²æ­¢æ— é™å¾ªç¯
                print(f"  âŒ è¶…è¿‡æœ€å¤§æ­¥æ•°é™åˆ¶")
                break

        # æ˜¾ç¤ºè·¯å¾„
        print(f"  è·¯å¾„: {' â†’ '.join(map(str, path))}")

    # ç»Ÿè®¡ç»“æœ
    success_rate = success_count / n_tests
    avg_steps = np.mean(total_steps) if total_steps else 0

    print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
    print(f"æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{n_tests})")
    if total_steps:
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
        print(f"æœ€å°‘æ­¥æ•°: {min(total_steps)}")

def visualize_learning_curves(episode_rewards: List[float],
                            episode_lengths: List[float],
                            td_errors: List[float]) -> None:
    """å¯è§†åŒ–å­¦ä¹ æ›²çº¿"""

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Q-Learningå­¦ä¹ è¿‡ç¨‹åˆ†æ', fontsize=16, fontweight='bold')

    episodes = range(len(episode_rewards))

    # 1. å›åˆå¥–åŠ±
    ax1 = axes[0, 0]
    ax1.plot(episodes, episode_rewards, alpha=0.6, linewidth=0.8)
    # æ·»åŠ æ»‘åŠ¨å¹³å‡
    window_size = 50
    if len(episode_rewards) > window_size:
        moving_avg = np.convolve(episode_rewards, np.ones(window_size)/window_size, mode='valid')
        ax1.plot(range(window_size-1, len(episode_rewards)), moving_avg,
                'r-', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size})')
        ax1.legend()

    ax1.set_xlabel('å›åˆ')
    ax1.set_ylabel('æ€»å¥–åŠ±')
    ax1.set_title('å›åˆå¥–åŠ±å˜åŒ–')
    ax1.grid(True, alpha=0.3)

    # 2. å›åˆé•¿åº¦
    ax2 = axes[0, 1]
    ax2.plot(episodes, episode_lengths, alpha=0.6, linewidth=0.8)
    # æ·»åŠ æ»‘åŠ¨å¹³å‡
    if len(episode_lengths) > window_size:
        moving_avg = np.convolve(episode_lengths, np.ones(window_size)/window_size, mode='valid')
        ax2.plot(range(window_size-1, len(episode_lengths)), moving_avg,
                'r-', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size})')
        ax2.legend()

    ax2.set_xlabel('å›åˆ')
    ax2.set_ylabel('æ­¥æ•°')
    ax2.set_title('å›åˆé•¿åº¦å˜åŒ–')
    ax2.grid(True, alpha=0.3)

    # 3. TDè¯¯å·®
    ax3 = axes[1, 0]
    ax3.plot(episodes, td_errors, alpha=0.6, linewidth=0.8)
    # æ·»åŠ æ»‘åŠ¨å¹³å‡
    if len(td_errors) > window_size:
        moving_avg = np.convolve(td_errors, np.ones(window_size)/window_size, mode='valid')
        ax3.plot(range(window_size-1, len(td_errors)), moving_avg,
                'r-', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size})')
        ax3.legend()

    ax3.set_xlabel('å›åˆ')
    ax3.set_ylabel('å¹³å‡TDè¯¯å·®')
    ax3.set_title('å­¦ä¹ è¿›åº¦ (TDè¯¯å·®)')
    ax3.grid(True, alpha=0.3)

    # 4. å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
    ax4 = axes[1, 1]
    ax4.hist(episode_rewards, bins=30, alpha=0.7, edgecolor='black')
    ax4.axvline(np.mean(episode_rewards), color='red', linestyle='--',
               label=f'å¹³å‡å€¼: {np.mean(episode_rewards):.2f}')
    ax4.legend()
    ax4.set_xlabel('æ€»å¥–åŠ±')
    ax4.set_ylabel('é¢‘æ¬¡')
    ax4.set_title('å¥–åŠ±åˆ†å¸ƒ')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case2_learning_curves.png',
                dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å­¦ä¹ æ›²çº¿å·²ä¿å­˜åˆ°: results/plots/case2_learning_curves.png")

    plt.show()

def compare_parameters() -> None:
    """å¯¹æ¯”ä¸åŒå‚æ•°è®¾ç½®çš„æ•ˆæœ"""
    print(f"\nğŸ”¬ å‚æ•°å¯¹æ¯”å®éªŒ")

    # ä¸åŒçš„å‚æ•°è®¾ç½®
    configs = [
        {'lr': 0.05, 'epsilon': 0.1, 'name': 'ä¿å®ˆå­¦ä¹ '},
        {'lr': 0.1, 'epsilon': 0.1, 'name': 'æ ‡å‡†è®¾ç½®'},
        {'lr': 0.2, 'epsilon': 0.1, 'name': 'æ¿€è¿›å­¦ä¹ '},
        {'lr': 0.1, 'epsilon': 0.3, 'name': 'é«˜æ¢ç´¢'},
    ]

    env = GridWorld(size=5)
    n_states = env.size * env.size
    episodes = 500

    results = {}

    for config in configs:
        print(f"\næµ‹è¯•é…ç½®: {config['name']} (Î±={config['lr']}, Îµ={config['epsilon']})")

        agent = QLearningAgent(n_states, env.n_actions,
                              learning_rate=config['lr'],
                              epsilon=config['epsilon'])

        episode_rewards = []

        for episode in range(episodes):
            pos = env.reset()
            state = env.get_state_id(pos)
            total_reward = 0
            steps = 0

            while True:
                action = agent.choose_action(state)
                next_pos, reward, done = env.step(action)
                next_state = env.get_state_id(next_pos)

                agent.update(state, action, reward, next_state, done)

                total_reward += reward
                steps += 1
                state = next_state

                if done or steps > 100:
                    break

            episode_rewards.append(total_reward)

        results[config['name']] = episode_rewards
        final_avg = np.mean(episode_rewards[-50:])
        print(f"  æœ€ç»ˆ50å›åˆå¹³å‡å¥–åŠ±: {final_avg:.2f}")

    # å¯è§†åŒ–å¯¹æ¯”ç»“æœ
    plt.figure(figsize=(12, 6))

    for name, rewards in results.items():
        # è®¡ç®—æ»‘åŠ¨å¹³å‡
        window_size = 50
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        episodes_range = range(window_size-1, len(rewards))
        plt.plot(episodes_range, moving_avg, linewidth=2, label=name)

    plt.xlabel('å›åˆ')
    plt.ylabel('å¹³å‡å¥–åŠ±')
    plt.title('ä¸åŒå‚æ•°è®¾ç½®çš„å­¦ä¹ æ•ˆæœå¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ä¿å­˜å›¾ç‰‡
    plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case2_parameter_comparison.png',
                dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—ºï¸  ç½‘æ ¼ä¸–ç•ŒQ-Learningï¼šç†è§£ä»·å€¼å‡½æ•°å­¦ä¹ ")
    print("=" * 55)

    # è¿è¡Œè®­ç»ƒ
    agent, episode_rewards, episode_lengths, td_errors = run_training(
        episodes=1000, visualize_every=200
    )

    # å¯è§†åŒ–æœ€ç»ˆç»“æœ
    print(f"\nğŸ“Š å¯è§†åŒ–æœ€ç»ˆå­¦ä¹ ç»“æœ")
    env = GridWorld(size=5)
    policy = agent.get_policy()
    fig = env.visualize(agent.q_table, policy)
    fig.suptitle('æœ€ç»ˆQ-Learningç»“æœ', fontsize=16, fontweight='bold')

    # ä¿å­˜æœ€ç»ˆç»“æœ
    plt.savefig('/Users/xifeng/project/finetuning-0106/experiments/RL/results/plots/case2_final_result.png',
                dpi=300, bbox_inches='tight')
    plt.show()

    # æµ‹è¯•Agent
    test_agent(agent, env, n_tests=3)

    # å¯è§†åŒ–å­¦ä¹ æ›²çº¿
    visualize_learning_curves(episode_rewards, episode_lengths, td_errors)

    # å‚æ•°å¯¹æ¯”å®éªŒ
    # try:
    #     user_input = input("\næ˜¯å¦è¦è¿›è¡Œå‚æ•°å¯¹æ¯”å®éªŒï¼Ÿ(y/n): ").strip().lower()
    #     if user_input == 'y':
    #         compare_parameters()
    # except KeyboardInterrupt:
    #     print("\nç”¨æˆ·å–æ¶ˆ")

    print("\nâœ… æ¡ˆä¾‹2å®Œæˆï¼")
    print("ğŸ“ ä½ å­¦åˆ°äº†ï¼š")
    print("  â€¢ Q-Learningå¦‚ä½•é€šè¿‡TDå­¦ä¹ æ›´æ–°ä»·å€¼å‡½æ•°")
    print("  â€¢ ç­–ç•¥å¦‚ä½•ä»ä»·å€¼å‡½æ•°ä¸­æå–")
    print("  â€¢ æ¢ç´¢ç‡å’Œå­¦ä¹ ç‡å¯¹å­¦ä¹ æ•ˆæœçš„å½±å“")
    print("  â€¢ ä»·å€¼å‡½æ•°çš„å¯è§†åŒ–å¸®åŠ©ç†è§£ç®—æ³•è¡Œä¸º")
    print("\nâ¡ï¸  ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python case3_cartpole.py ä½“éªŒæ·±åº¦å¼ºåŒ–å­¦ä¹ ")

if __name__ == "__main__":
    main()