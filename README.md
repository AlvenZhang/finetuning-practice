# ğŸš€ LLMå¾®è°ƒé¡¹ç›® - MacBook Pro M3 Pro

ä¸€ä¸ªä¸“ä¸ºMacBook Pro M3 Pro (18GBå†…å­˜) ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒå®éªŒé¡¹ç›®ï¼Œä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡Œå‚æ•°é«˜æ•ˆçš„æŒ‡ä»¤å¾®è°ƒã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡å®è·µå­¦ä¹ å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒæ¦‚å¿µå’ŒæŠ€æœ¯ï¼Œç‰¹åˆ«é’ˆå¯¹Apple Siliconç¡¬ä»¶è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Llama 3.2-3Bæ¨¡å‹å’ŒAlpacaæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡çš„å¾®è°ƒã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- ç†è§£å‚æ•°é«˜æ•ˆå¾®è°ƒ(LoRA)æŠ€æœ¯
- æŒæ¡æŒ‡ä»¤å¾®è°ƒå’Œå¯¹é½æŠ€æœ¯
- å­¦ä¹ å…¨é¢çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•
- æŒæ¡Apple Siliconä¼˜åŒ–æŠ€æœ¯

### ğŸ”§ æŠ€æœ¯æ ˆ
- **æ¨¡å‹**: Llama 3.2-3B Instruct
- **å¾®è°ƒæ–¹æ³•**: LoRA (Low-Rank Adaptation)
- **æ•°æ®é›†**: Alpaca-GPT4 (52Kæ ·æœ¬)
- **ä¼˜åŒ–æ¡†æ¶**: MLX (Apple Siliconä¼˜åŒ–)
- **ä»»åŠ¡ç±»å‹**: æŒ‡ä»¤è·Ÿéš/å¯¹è¯è¡¥å…¨

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
/Users/xifeng/project/finetuning-0106/
â”œâ”€â”€ README.md                     # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â”œâ”€â”€ config/                       # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ model_config.yaml         # æ¨¡å‹å’Œè®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ lora_config.yaml          # LoRAä¸“ç”¨è®¾ç½®
â”‚   â””â”€â”€ eval_config.yaml          # è¯„ä¼°è®¾ç½®
â”œâ”€â”€ data/                         # æ•°æ®é›†ç®¡ç†
â”‚   â”œâ”€â”€ raw/                      # åŸå§‹æ•°æ®é›†
â”‚   â”œâ”€â”€ processed/                # æ¸…æ´—æ ¼å¼åŒ–æ•°æ®
â”‚   â””â”€â”€ download_data.py          # æ•°æ®ä¸‹è½½è„šæœ¬
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ data/                     # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ model/                    # æ¨¡å‹ç›¸å…³æ¨¡å—
â”‚   â”œâ”€â”€ training/                 # è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ evaluation/               # è¯„ä¼°æ¨¡å—
â”‚   â””â”€â”€ utils/                    # å·¥å…·æ¨¡å—
â”œâ”€â”€ scripts/                      # å¯æ‰§è¡Œè„šæœ¬
â”œâ”€â”€ notebooks/                    # Jupyteråˆ†æç¬”è®°æœ¬
â”œâ”€â”€ experiments/                  # å®éªŒè·Ÿè¸ª
â”œâ”€â”€ models/                       # æ¨¡å‹å­˜å‚¨
â””â”€â”€ docs/                         # æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›® (å¦‚æœä»git)
git clone <repository-url>
cd finetuning-0106

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # macOS/Linux

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½®Hugging Face Token

```bash
# ç™»å½•Hugging Face (éœ€è¦è®¿é—®Llamaæ¨¡å‹)
huggingface-cli login
```

### 3. ä¸‹è½½æ•°æ®å’Œæ¨¡å‹

```bash
# ä¸‹è½½Alpacaæ•°æ®é›†
python data/download_data.py

# ä¸‹è½½åŸºç¡€æ¨¡å‹
python scripts/download_model.py
```

### 4. å¼€å§‹è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¼€å§‹è®­ç»ƒ
python scripts/train.py

# æˆ–è€…ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python scripts/train.py --config config/model_config.yaml
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (model_config.yaml)
- æ¨¡å‹é€‰æ‹©å’ŒåŠ è½½è®¾ç½®
- è®­ç»ƒè¶…å‚æ•°
- æ•°æ®å¤„ç†é…ç½®
- è·¯å¾„è®¾ç½®

### LoRAé…ç½® (lora_config.yaml)
- LoRAå‚æ•°è®¾ç½® (rank=16, alpha=32)
- ç›®æ ‡æ¨¡å—é…ç½®
- å†…å­˜ä¼˜åŒ–è®¾ç½®

### è¯„ä¼°é…ç½® (eval_config.yaml)
- è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡
- äººå·¥è¯„ä¼°è®¾ç½®
- å®éªŒè·Ÿè¸ªé…ç½®

## ğŸ“Š é¢„æœŸæˆæœ

### æ€§èƒ½æŒ‡æ ‡
- **æŒ‡ä»¤è·Ÿéšæ”¹è¿›**: 15-25% vs åŸºçº¿
- **è®­ç»ƒæ—¶é—´**: 4-6å°æ—¶
- **å†…å­˜ä½¿ç”¨**: å³°å€¼~14GB
- **æ¨ç†é€Ÿåº¦**: 20-30 tokens/ç§’

### è¾“å‡ºæ–‡ä»¶
- è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨
- è¯„ä¼°æŠ¥å‘Šå’ŒæŒ‡æ ‡
- è®­ç»ƒæ—¥å¿—å’Œå¯è§†åŒ–
- æ ·æœ¬è¾“å‡ºå¯¹æ¯”

## ğŸ”§ å†…å­˜ä¼˜åŒ–ç­–ç•¥

é’ˆå¯¹M3 Pro 18GBå†…å­˜çš„ä¼˜åŒ–æªæ–½ï¼š

1. **LoRAå¾®è°ƒ**: å‡å°‘99%+å¯è®­ç»ƒå‚æ•°
2. **æ··åˆç²¾åº¦**: FP16è®­ç»ƒèŠ‚çœå†…å­˜
3. **æ¢¯åº¦ç´¯ç§¯**: å°æ‰¹æ¬¡å®ç°å¤§æœ‰æ•ˆæ‰¹å¤§å°
4. **æ¢¯åº¦æ£€æŸ¥ç‚¹**: ç”¨è®¡ç®—æ¢å†…å­˜
5. **MLXä¼˜åŒ–**: åŸç”ŸApple SiliconåŠ é€Ÿ

## ğŸ“ˆ å®éªŒè·Ÿè¸ª

### Weights & Biases
```python
# é…ç½®wandb
wandb.init(
    project="llm-finetuning",
    tags=["llama-3.2-3B", "lora", "alpaca", "m3-pro"]
)
```

### æœ¬åœ°æ—¥å¿—
- è®­ç»ƒæ—¥å¿—: `experiments/logs/`
- æ¨¡å‹æ£€æŸ¥ç‚¹: `models/checkpoints/`
- è¯„ä¼°ç»“æœ: `experiments/results/`

## ğŸ“š ä½¿ç”¨æŒ‡å—

### è®­ç»ƒè„šæœ¬
```bash
# åŸºç¡€è®­ç»ƒ
python scripts/train.py

# è‡ªå®šä¹‰LoRAå‚æ•°
python scripts/train.py --lora_r 32 --lora_alpha 64

# æ¢å¤è®­ç»ƒ
python scripts/train.py --resume_from_checkpoint models/checkpoints/checkpoint-1000
```

### è¯„ä¼°è„šæœ¬
```bash
# è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
python scripts/evaluate.py --model_path models/final/

# å¯¹æ¯”è¯„ä¼°
python scripts/evaluate.py --compare_baseline
```

### æ¨ç†è„šæœ¬
```bash
# äº¤äº’å¼æ¨ç†
python scripts/inference.py --model_path models/final/

# æ‰¹é‡æ¨ç†
python scripts/inference.py --input_file test_prompts.txt --output_file results.txt
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   - å‡å°‘batch_size
   - å¢åŠ gradient_accumulation_steps
   - é™ä½max_length

2. **è®­ç»ƒé€Ÿåº¦æ…¢**
   - ç¡®ä¿å¯ç”¨MLXä¼˜åŒ–
   - æ£€æŸ¥FP16è®¾ç½®
   - éªŒè¯gradient_checkpointingé…ç½®

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥Hugging Face token
   - éªŒè¯æ¨¡å‹è·¯å¾„
   - ç¡®è®¤ç½‘ç»œè¿æ¥

### æ€§èƒ½ç›‘æ§
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
python -c "
import psutil
print(f'Memory: {psutil.virtual_memory().percent}%')
print(f'Available: {psutil.virtual_memory().available/1024**3:.1f}GB')
"

# ç›‘æ§GPUä½¿ç”¨ (å¦‚æœé€‚ç”¨)
python -c "
import torch
if torch.backends.mps.is_available():
    print('MPS (Metal Performance Shaders) available')
"
```

## ğŸ“– å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»
- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [Alpacaè®ºæ–‡](https://arxiv.org/abs/2303.16199)
- [MLXæ–‡æ¡£](https://ml-explore.github.io/mlx/build/html/index.html)

### ç›¸å…³é¡¹ç›®
- [Hugging Face PEFT](https://github.com/huggingface/peft)
- [MLX Examples](https://github.com/ml-explore/mlx-examples)

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»ºPull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - æŸ¥çœ‹[LICENSE](LICENSE)æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- Hugging Faceå›¢é˜Ÿæä¾›çš„transformersåº“
- Appleçš„MLXå›¢é˜Ÿ
- Stanford Alpacaé¡¹ç›®
- Metaçš„Llamaæ¨¡å‹

---

**Happy Fine-tuning! ğŸ‰**

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·åˆ›å»ºIssueæˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚